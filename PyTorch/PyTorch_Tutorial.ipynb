{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWmC7zvLNsDEjDDmNQ6m/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourish168/Python/blob/main/PyTorch/PyTorch_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction and Installation\n",
        "- From https://pytorch.org/ we can download the required package of PyTorch in our machine(Linux/Mac/Windows).\n",
        "- If gpu is available then the cuda version is preferable for NVIDIA GPUs only.\n",
        "- In local mechine we have to create a virtual environment and install the appropiate version of python.\n",
        "- In Google Colab we have to install PyTorch by \"!pip install torch\" command.\n",
        "- By \"torch.cuda.is_available()\" we can check if the cuda is there or not.\n",
        "- We can use PyTorch documentation for our specific purpose. Link: https://pytorch.org/docs/stable/index.html"
      ],
      "metadata": {
        "id": "ugEeAVTRoSyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5AKWDZRYma0V",
        "outputId": "96b7fa88-eb57-45b0-a284-588057fe61ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available() # False means there is no cuda in the machine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_9FDd0q7yip",
        "outputId": "f43772ee-b4a5-4405-83cf-7232bd313c42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Basics\n",
        "- In NumPy we have used vectors, matrices, tensors. In PyTorch everything starts with tensors i.e. 1D, 2D, nD arrays."
      ],
      "metadata": {
        "id": "p0bRM3vE8Nek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.empty(1) # Scalar\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qKcD9KK8Nyq",
        "outputId": "d6da6b15-f463-4f84-86b2-b7107de8efc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.8665e+32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(2, 3, 4) # 3D array\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD1jYWu989KS",
        "outputId": "5a3316f5-39c4-4e06-9a5a-d5f64f2b0004"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 5.7344e-30,  4.4355e-41,  5.7344e-30,  4.4355e-41],\n",
            "         [        nan,  0.0000e+00,  1.8728e+31,  1.4153e-43],\n",
            "         [ 3.2892e+14,  4.4354e-41,  0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 8.9683e-44,  0.0000e+00,  2.6905e-43,  0.0000e+00],\n",
            "         [-6.3723e+09,  4.4354e-41,  0.0000e+00,  1.4013e-45],\n",
            "         [ 9.2327e-26,  3.2695e-41,  0.0000e+00,  0.0000e+00]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros(2, 2, 3) # 3D array of zeros\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjUbfKlg9Owk",
        "outputId": "9661b74f-b804-4bb3-feba-e7a2b80835c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(2, 3, dtype = torch.float16) # 2D array of ones\n",
        "print(x)\n",
        "print(\"\\n\", x.dtype) # Prints the datatype of x\n",
        "print(\"\\n\", x.size()) # Prints the size of x\n",
        "print(\"\\n\", x.shape) # Prints the shape of x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmXwva4a9bEe",
        "outputId": "72b7ea68-5868-4a07-8f14-7ff8522f2fd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float16)\n",
            "\n",
            " torch.float16\n",
            "\n",
            " torch.Size([2, 3])\n",
            "\n",
            " torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([2.5, 0.1, 3.6]) # Creates a tensor from a list\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBavwzZN-NoM",
        "outputId": "d19fb6e7-a41b-4320-dc23-25cbad068dc9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.5000, 0.1000, 3.6000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2, 2) # 2D array of random values\n",
        "y = torch.rand(2, 2)\n",
        "print(\"x =\\n\", x)\n",
        "print(\"\\ny =\\n\", y)\n",
        "print(\"\\nx+y =\\n\", x+y) # Addition\n",
        "print(\"\\ntorch.add(x, y) =\\n\", torch.add(x, y)) # Addition\n",
        "print(\"\\nx-y =\\n\", x-y) # Subtraction\n",
        "print(\"\\ntorch.sub(x, y) =\\n\", torch.sub(x, y)) # Subtraction\n",
        "print(\"\\nx*y =\\n\", x*y) # Multiplication\n",
        "print(\"\\ntorch.mul(x, y) =\\n\", torch.mul(x, y)) # Multiplication\n",
        "print(\"\\nx/y =\\n\", x/y) # Division\n",
        "print(\"\\ntorch.div(x, y) =\\n\", torch.div(x, y)) # Division"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rmI5Npm-xzy",
        "outputId": "8f49f8ef-d1e5-49c1-c006-8cb3b1dbb056"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x =\n",
            " tensor([[0.7870, 0.4952],\n",
            "        [0.1715, 0.4954]])\n",
            "\n",
            "y =\n",
            " tensor([[0.5973, 0.2423],\n",
            "        [0.5274, 0.0733]])\n",
            "\n",
            "x+y =\n",
            " tensor([[1.3843, 0.7376],\n",
            "        [0.6989, 0.5687]])\n",
            "\n",
            "torch.add(x, y) =\n",
            " tensor([[1.3843, 0.7376],\n",
            "        [0.6989, 0.5687]])\n",
            "\n",
            "x-y =\n",
            " tensor([[ 0.1896,  0.2529],\n",
            "        [-0.3559,  0.4221]])\n",
            "\n",
            "torch.sub(x, y) =\n",
            " tensor([[ 0.1896,  0.2529],\n",
            "        [-0.3559,  0.4221]])\n",
            "\n",
            "x*y =\n",
            " tensor([[0.4701, 0.1200],\n",
            "        [0.0904, 0.0363]])\n",
            "\n",
            "torch.mul(x, y) =\n",
            " tensor([[0.4701, 0.1200],\n",
            "        [0.0904, 0.0363]])\n",
            "\n",
            "x/y =\n",
            " tensor([[1.3175, 2.0436],\n",
            "        [0.3251, 6.7598]])\n",
            "\n",
            "torch.div(x, y) =\n",
            " tensor([[1.3175, 2.0436],\n",
            "        [0.3251, 6.7598]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2, 2)\n",
        "y = torch.rand(2, 2)\n",
        "print(\"x =\\n\", x)\n",
        "print(\"\\ny =\\n\", y)\n",
        "print(\"\\nx+y =\\n\", x+y) # Addition\n",
        "y.add_(x) # Inplace addition\n",
        "print(\"\\nNow, y =\\n\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuDBvpal_Ji4",
        "outputId": "5e6c71dd-97fb-4d20-b0e9-9ce027e29e07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x =\n",
            " tensor([[0.6547, 0.3169],\n",
            "        [0.1159, 0.8991]])\n",
            "\n",
            "y =\n",
            " tensor([[0.6328, 0.8502],\n",
            "        [0.6011, 0.8069]])\n",
            "\n",
            "x+y =\n",
            " tensor([[1.2875, 1.1671],\n",
            "        [0.7170, 1.7060]])\n",
            "\n",
            "Now, y =\n",
            " tensor([[1.2875, 1.1671],\n",
            "        [0.7170, 1.7060]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing of a tensor in PyTorch\n",
        "x = torch.rand(5, 4)\n",
        "print(x)\n",
        "print(\"\\n\", x[:, 0]) # Prints the first column\n",
        "print(\"\\n\", x[1, :]) # Prints the second row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zml4ct74BSTi",
        "outputId": "874da762-ebe1-4740-ef9f-48dd6bca1fca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2758, 0.0263, 0.4842, 0.3037],\n",
            "        [0.9383, 0.4854, 0.5855, 0.4944],\n",
            "        [0.5045, 0.8050, 0.6152, 0.1146],\n",
            "        [0.6972, 0.2365, 0.1701, 0.5679],\n",
            "        [0.1362, 0.5831, 0.3701, 0.5951]])\n",
            "\n",
            " tensor([0.2758, 0.9383, 0.5045, 0.6972, 0.1362])\n",
            "\n",
            " tensor([0.9383, 0.4854, 0.5855, 0.4944])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping of a tensor in PyTorch\n",
        "x = torch.rand(4, 4)\n",
        "print(\"x =\", x)\n",
        "y = x.view(2, 8) # Reshapes the tensor to 2D of shape (2, 8)\n",
        "print(\"\\ny =\", y)\n",
        "z = x.view(16) # Reshapes the tensor to 1D of shape (16)\n",
        "print(\"\\nz =\", z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcKp7sX0B3Hh",
        "outputId": "8edd6946-3fb5-49dc-9c5c-b99e9d430379"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([[0.9847, 0.5975, 0.7559, 0.1402],\n",
            "        [0.5898, 0.6594, 0.3962, 0.7136],\n",
            "        [0.9404, 0.0475, 0.6655, 0.4802],\n",
            "        [0.1402, 0.3459, 0.9550, 0.7598]])\n",
            "\n",
            "y = tensor([[0.9847, 0.5975, 0.7559, 0.1402, 0.5898, 0.6594, 0.3962, 0.7136],\n",
            "        [0.9404, 0.0475, 0.6655, 0.4802, 0.1402, 0.3459, 0.9550, 0.7598]])\n",
            "\n",
            "z = tensor([0.9847, 0.5975, 0.7559, 0.1402, 0.5898, 0.6594, 0.3962, 0.7136, 0.9404,\n",
            "        0.0475, 0.6655, 0.4802, 0.1402, 0.3459, 0.9550, 0.7598])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4, 4)\n",
        "print(\"x =\", x)\n",
        "y = x.view(-1, 8) # Reshapes the tensor to 2D of shape (2, 8), Negative value by default choose the proper dimension\n",
        "print(\"\\ny =\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEamdPLhCNH_",
        "outputId": "913f3073-29f3-452c-f065-fc7112e62736"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([[0.0124, 0.9629, 0.6070, 0.7891],\n",
            "        [0.5116, 0.8248, 0.1646, 0.9194],\n",
            "        [0.5410, 0.4994, 0.0865, 0.5310],\n",
            "        [0.9660, 0.4909, 0.0985, 0.1097]])\n",
            "\n",
            "y = tensor([[0.0124, 0.9629, 0.6070, 0.7891, 0.5116, 0.8248, 0.1646, 0.9194],\n",
            "        [0.5410, 0.4994, 0.0865, 0.5310, 0.9660, 0.4909, 0.0985, 0.1097]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If there is no GPU memory then PyTorch and NumPy shares the same memory location thus the output changes\n",
        "import numpy as np\n",
        "a = torch.ones(5)\n",
        "print(\"a =\", a)\n",
        "b = a.numpy() # Creates a numpy array from a tensor\n",
        "print(\"\\nb =\", b)\n",
        "print(\"\\nType of a =\", type(a))\n",
        "print(\"\\nType of b =\", type(b))\n",
        "a.add_(1)\n",
        "print(\"\\nNow, a =\", a)\n",
        "print(\"\\nNow, b =\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gffkflHDDffz",
        "outputId": "ac6533c6-a794-45a0-daa0-3754bc4e7179"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = tensor([1., 1., 1., 1., 1.])\n",
            "\n",
            "b = [1. 1. 1. 1. 1.]\n",
            "\n",
            "Type of a = <class 'torch.Tensor'>\n",
            "\n",
            "Type of b = <class 'numpy.ndarray'>\n",
            "\n",
            "Now, a = tensor([2., 2., 2., 2., 2.])\n",
            "\n",
            "Now, b = [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.ones(5)\n",
        "print(\"a =\", a)\n",
        "b = torch.from_numpy(a) # Creates a tensor from a numpy array\n",
        "print(\"\\nb =\", b)\n",
        "print(\"\\nType of a =\", type(a))\n",
        "print(\"\\nType of b =\", type(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2yMaMkrECWk",
        "outputId": "429b3e81-f0a3-4727-b4fe-79b35f971afa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = [1. 1. 1. 1. 1.]\n",
            "\n",
            "b = tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "\n",
            "Type of a = <class 'numpy.ndarray'>\n",
            "\n",
            "Type of b = <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since in this case, torch.cuda.is_available() = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\") # Creates a device object\n",
        "  print(device)\n",
        "  x = torch.ones(5, device = device) # Creates a tensor on the GPU\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device) # Moves the tensor to the GPU\n",
        "  z = x + y\n",
        "  print(z)\n",
        "  z = z.to(\"cpu\") # Moves the tensor to the CPU\n",
        "  print(z)\n",
        "else:\n",
        "  print(\"No GPU available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvjH0MZdFHq9",
        "outputId": "9083337c-5773-403e-8d05-2fee31b490cd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(5, requires_grad = True) # Creates a tensor with requires_grad = True\n",
        "print(x) # Later for optimization steps the tensor needs to calculate the gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbolIfuXFnya",
        "outputId": "722b1802-f802-4958-a6ae-4d70df2857b2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Calculation with Autograd in PyTorch\n",
        "- In Deep Learning, gradient calculation is a very crutial step for optimization."
      ],
      "metadata": {
        "id": "9uY4IGU6GaOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.randn(3, requires_grad = True) # Creates a tensor with requires_grad = True\n",
        "print(\"x =\", x)\n",
        "y = x + 2 # Creates a tensor with requires_grad = False\n",
        "print(\"\\ny =\", y)\n",
        "z = y*y*2 # Creates a tensor with requires_grad = False\n",
        "print(\"\\nz =\", z)\n",
        "p = z.mean() # Creates a tensor with requires_grad = False\n",
        "print(\"\\np =\", p)\n",
        "p.backward() # Calculates the gradient of p with respect to x i.e. dp/dx\n",
        "print(\"\\nx.grad =\", x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsDXjwJzGFFG",
        "outputId": "cd836947-7965-48a7-af4a-ffebf7522a19"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([0.5580, 0.4624, 0.7128], requires_grad=True)\n",
            "\n",
            "y = tensor([2.5580, 2.4624, 2.7128], grad_fn=<AddBackward0>)\n",
            "\n",
            "z = tensor([13.0869, 12.1266, 14.7182], grad_fn=<MulBackward0>)\n",
            "\n",
            "p = tensor(13.3106, grad_fn=<MeanBackward0>)\n",
            "\n",
            "x.grad = tensor([3.4107, 3.2832, 3.6170])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad = False) # Creates a tensor with requires_grad = True\n",
        "print(\"x =\", x)\n",
        "y = x + 2 # Creates a tensor with requires_grad = False\n",
        "print(\"\\ny =\", y)\n",
        "z = y*y*2 # Creates a tensor with requires_grad = False\n",
        "print(\"\\nz =\", z)\n",
        "p = z.mean() # Creates a tensor with requires_grad = False\n",
        "print(\"\\np =\", p)\n",
        "p.backward() # Calculates the gradient of p with respect to x i.e. dp/dx\n",
        "print(\"\\nx.grad =\", x.grad) # Since requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "wREQhwl0ICWU",
        "outputId": "4233acd3-bba1-4f9f-c1e8-fba35a23b672"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([-1.5262,  0.2790,  1.0887])\n",
            "\n",
            "y = tensor([0.4738, 2.2790, 3.0887])\n",
            "\n",
            "z = tensor([ 0.4490, 10.3881, 19.0807])\n",
            "\n",
            "p = tensor(9.9726)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ff0155360e09>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Creates a tensor with requires_grad = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\np =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculates the gradient of p with respect to x i.e. dp/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nx.grad =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Since requires_grad = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad = True) # Creates a tensor with requires_grad = True\n",
        "print(\"x =\", x)\n",
        "y = x + 2 # Creates a tensor with requires_grad = False\n",
        "print(\"\\ny =\", y)\n",
        "z = y*y*2 # Creates a tensor with requires_grad = False\n",
        "print(\"\\nz =\", z)\n",
        "v = torch.tensor([0.1, 10.0, 0.001], dtype = torch.float32)\n",
        "z.backward(v) # Calculates the gradient of p with respect to x i.e. dp/dx\n",
        "print(\"\\nx.grad =\", x.grad) # Vector Jacobian Product in the background"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5tsCCNuJBaH",
        "outputId": "e0c68661-553e-4def-fbd9-c69aaaea0b32"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([-0.0687, -0.8293, -0.0447], requires_grad=True)\n",
            "\n",
            "y = tensor([1.9313, 1.1707, 1.9553], grad_fn=<AddBackward0>)\n",
            "\n",
            "z = tensor([7.4598, 2.7409, 7.6466], grad_fn=<MulBackward0>)\n",
            "\n",
            "x.grad = tensor([7.7252e-01, 4.6826e+01, 7.8213e-03])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preventing gradient history\n",
        "x = torch.randn(3, requires_grad = True) # Creates a tensor with requires_grad = True\n",
        "print(\"x =\", x)\n",
        "x.requires_grad_(False) # Prevents gradient history\n",
        "print(\"\\nx =\", x)\n",
        "print(\"\\n\", 50*\"-\")\n",
        "x = torch.randn(3, requires_grad = True) # Creates a tensor with requires_grad = True\n",
        "print(\"\\nx =\", x)\n",
        "y = x.detach() # Detaches the gradient history\n",
        "print(\"\\ny =\", y)\n",
        "print(\"\\n\", 50*\"-\")\n",
        "x = torch.randn(3, requires_grad = True) # Creates a tensor with requires_grad = True\n",
        "print(\"\\nx =\", x)\n",
        "with torch.no_grad(): # Prevents gradient history\n",
        "  y = x + 2\n",
        "  print(\"\\ny =\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_GgOULiKfs-",
        "outputId": "dd16a121-5935-49fa-e6a2-58badd48f764"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([ 0.1956, -0.0141, -0.1218], requires_grad=True)\n",
            "\n",
            "x = tensor([ 0.1956, -0.0141, -0.1218])\n",
            "\n",
            " --------------------------------------------------\n",
            "\n",
            "x = tensor([0.6713, 0.0724, 0.5710], requires_grad=True)\n",
            "\n",
            "y = tensor([0.6713, 0.0724, 0.5710])\n",
            "\n",
            " --------------------------------------------------\n",
            "\n",
            "x = tensor([ 0.7072, -0.6595, -0.1999], requires_grad=True)\n",
            "\n",
            "y = tensor([2.7072, 1.3405, 1.8001])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy Trainning Example\n",
        "weight = torch.ones(4, requires_grad = True)\n",
        "print(\"x =\", weight, \"\\n\")\n",
        "for epoch in range(3):\n",
        "  model_output = (weight*3).sum() # Dummy model output\n",
        "  print(model_output)\n",
        "  model_output.backward()\n",
        "  print(weight.grad)\n",
        "  weight.grad.zero_() # Resets the gradient to zero, optimization step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8PbZL25Mx98",
        "outputId": "8d84185d-5fd7-4bfa-c118-126e572ddffb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([1., 1., 1., 1.], requires_grad=True) \n",
            "\n",
            "tensor(12., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor(12., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor(12., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization Steps\n",
        "weight = torch.ones(4, requires_grad=True)\n",
        "print(\"x =\", weight)\n",
        "optimizer = torch.optim.SGD([weight], lr=0.01)  # Pass [weight] as a single-element list, Stochastic Gradient Descent\n",
        "optimizer.step()  # Optimization step\n",
        "optimizer.zero_grad()  # Resets the gradient to zero, optimization step\n",
        "print(\"\\nOptimizer =\", optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tikk5tjzNUF3",
        "outputId": "53e8f618-692e-4469-af2b-063060a1a8b5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([1., 1., 1., 1.], requires_grad=True)\n",
            "\n",
            "Optimizer = SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation in PyTorch\n",
        "- To update the weights in the input of neurons of our neural network, backpropagation is needed.\n",
        "- The update rule is: wˡᵢⱼ <- (wˡᵢⱼ - η*δϵₖ/δwˡᵢⱼ), i.e. wˡᵢⱼ -= η(δϵₖ/δwˡᵢⱼ)\n",
        "- Here,\n",
        "1. wˡᵢⱼ is the weight of the input at lth layer from ith node of the (l-1)th layer to jth node of lth layer.\n",
        "2. ϵₖ = ( ̂yₖ - yₖ) be the difference between predicted output ̂yₖ and actual output yₖ of kth datapoint. Sometimes, ϵₖ is called the error function or loss function and denoted as f( ̂yₖ, yₖ), a function to find the error between actual and predicted output.\n",
        "3. η be the learning rate in which the convergence will occur. Generally we take 0 < η < 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fibbBN5yPrBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz # To visualize the computational graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaMTVTZQlYZd",
        "outputId": "8782e3bc-2ac1-425c-d595-8a3dff1d97cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.3.1+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchviz)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchviz)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchviz)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchviz)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchviz)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchviz)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->torchviz)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=ae1a7afe45e00c6407721b373f7d4f20e437c3ae24ea7ce1f369a9d1093d398e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot # To visualize the computational graph\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "w = torch.tensor(1.0, requires_grad = True)\n",
        "# Forward pass and compute the loss\n",
        "y_hat = w*x\n",
        "loss = (y_hat - y)**2 # ϵₖ = f( ̂yₖ, yₖ) = ( ̂yₖ - yₖ)ᵅ, α = 2\n",
        "print(\"loss =\", loss)\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "print(\"\\nw.grad =\", w.grad)\n",
        "\n",
        "# Update weights\n",
        "# Next forward and backward pass\n",
        "# Update weights\n",
        "# Next forward and backward pass\n",
        "# And so on...\n",
        "\n",
        "\n",
        "# Visualize the computational graph\n",
        "dot = make_dot(loss, params={\"w\": w})\n",
        "dot.format = 'png'  # Set the format to PNG (you can choose other formats as well)\n",
        "dot.render(\"computational_graph\")  # Save the graph as 'computational_graph.png'\n",
        "\n",
        "# Display the graph in the notebook\n",
        "from IPython.display import Image\n",
        "Image(filename=\"computational_graph.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "7t--R8-nhfqR",
        "outputId": "f80d3e07-2caa-47e9-a850-d00018eec666"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = tensor(1., grad_fn=<PowBackward0>)\n",
            "\n",
            "w.grad = tensor(-2.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJEAAAGxCAIAAADtYcP2AAAABmJLR0QA/wD/AP+gvaeTAAAbEUlEQVR4nO2deVxTZ7rHnwRCZN9UBA1UFivSWBUR2RSUKlRQVBbDErWL3vb29iOiI9qZot0+va3VunWzzh2sOoNaqQKiUm1dPmp1RqFxV7BFpLgi4MYScv84M5k0JjHgyYnPO8/3r+Q9J+c8J9/P+57k5OT9iTQaDRCoEFu7AKLLkDN8kDN82Aq5s82bNwu5OyGJjIzs16+fQDvTCIhAh2QNioqKBHsbBe1nAJC3/IvIxIkC79TSTB3oI+Tu6HyGD3KGD3KGD3KGD3KGD3KGD3KGD6acLcxIPly+Q7flm6XvrX1nkbXqsRBMOesXGHT1crVuS92li75Bz1qrHgvBlDNZ4ID6y9UA8Mb4qGVz/wsA6mou+gYNtHZdPMOYs2frf6mpPv2z34DgX86dud/SfK2uVjaANWdCX2+0KLLAAdfran+qKA8bO97Z3f37rZvce/ZycnG1dl08w1Q/6+ndt729/eSBfcPjXggbM37ft0Uy5gZGYMwZAPTzD3R0dXVydRscEXOjvs6XuYERGBsbAeDDzWXcA4lUuvHEResWYyFY62f/CZAzfJAzfJAzfLDprL219X8SY36qKNdt3PDJ+//7xkvWKolH2HRWtOYTvwHB4S8k6jZm/M/82gvnju4ps1ZVfMGgs5Y7jeUb/i/t9Vy9domdXcorr29evQz7XXsMOjuyu7Svf6Dfs8GPLho1cerVy5d+OXta+Kp4hEFnp48dDgmLMLhI2sM+UD7kzD9+ErgkfmHQ2a2G33r69DW2tJdPv5v1V4Wsh3cYdPbg3l0HJydjSx1dXB/caxGyHt5h0Jm9o9P9u3eNLb3X3OTg5CJkPbzDoDPPPj4mRr8b9XWe3oLeXs87DDp7LjzyzN+PGlzU9vDhJVXloOEjBS6JXxh0FjE+qa76Yu3Fc48u2r/j277+gf2DQ4SvikcYdObk6paYPXPz6mV67e1tbd99vSb9v+dapSoeYdAZAKS/PvfXC2ePVuzUbSxa9bFv0LMjx02wVlV8wdrv1Bx2PXqsKj+o15id95ZViuEdNvsZ25AzfJAzfJAzfAj9GeR85T8E3iODCDarBfZfGk0j5PwgIlbfSpFIVFRUlJ6ebu1C+IfOZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/hg5z+DsbGx+/fvN7bUxsbmypUr3t7eQpZkIdjpZwqFwtgikUgUHR3NhjBgyVlaWpqtreG/9IvFYqVSKXA9loMdZx4eHvHx8Qa1iUSilJQU4UuyEOw4A4Ds7OzOzk69Rltb2xdffNHDw8MqJVkCppylpKTY2dnpNarV6uzsbKvUYyGYcubo6Dhx4kSJRKLb2KNHjwkT0M//pwtTzgAgKyurvb1d+1QikUydOtXBwcGKJfEOa84SExNdXP49rXd7e3tmZqYV67EErDmTSCTp6ena4dHV1TU+Pt66JfEOa84AIDMzkxseJRJJVlaW3umNAdi5dqWls7OzT58+N27cAIADBw7ExMRYuyKeYbCficXinJwcAOjTp09UVJS1y+Gf3101qKurO3z4sLVK4ZGePXsCwIgRI7Zu3WrtWnhAJpNFROgkTelODFhUVGS9wgijpKam6moycHVOpVIJXxbvlJeXJyYmPn69p568vDy9FgbPZxxsCDMIs84Yhpzhg5zhgwVn+/btk8vlcrl87lw0mUurVq364x//2L3XdnN+/Y6OjtTUVGdn52+++aZ7W+CRMWPGqFSqdevWnT5tbojx+vXrJRKJiVtIdOns7Ny2bduBAwc6OzsHDRqUk5Pj7Oz8BPU+Kd3sZwcPHhwxYoRYLK6treW3IGG4cOGC+SuvXLly69ateXl5H374YUtLy+LFiy1Wl1l001lpaenYsWMTEhJ27vx3xtitW7cWLlwYExMTFRU1f/78pqYm0+2TJ0/et28f9/jLL7+cP38+AJSVleXl5aWkpEyaNKm8vJx7iYn1TXDp0iWlUhkWFhYbG7tmzRpte0ZGxvbt2z/44ANuRL18+TLXfuPGjby8vIiIiKioqOXLl3O3KTx48GDDhg25ubl+fn5OTk4LFiz45JNPuPVLSkpmz5594sSJ5OTkIUOGvPPOO6b3W11dPW3atOHDh8+cOfPmzZtdfMv/TXec3b179+effw4LCxszZoyus/z8/I6Oju3bt5eVlXV0dGiPzVi7QSQSycGDB1esWCGVSrdv375x48bdu3e3tHQnnXjlypUhISGHDh1atWrV119/ferUKa69qKgoPDx80aJFKpVKpVL179+fa1+4cKFard65c+eWLVuOHDny17/+FQCqq6tbW1uDgw2kyvv5+Z09e3b16tUffPDB8ePHtV9+je33nXfeGTBgwI8//pibm/v9999344g4unM+27NnT3R0tFgs9vLycnJyOnPmzKBBg5qamo4ePVpaWsrdLbN8+XJuZWPtJggICPDz8wsICPD393/mmWfs7e2bmpq6cQpZuXIl90Aul/v6+tbV1T333HPGVm5paTl27FhxcbG7u7u7u/uMGTM2bdqUlZV17949AHBwcLh//354eDgAiMXiqqoqAHB1dW1sbHzppZfkcjkAaH/0Mbjf9vb2kydPFhQUODk5DR48ODY2tquHo6U7zkpLS48fP669/FpWVjZo0KA7d+7Avy7O6mKs3QSOjo4AYGNj06NHDwAQi8WP3k1lDhUVFWvXrq2trX348KFarTb9q1Nzc7NGo9G9pa53794A4O7uDgBNTU2enp4qlercuXMZGRncCiKRCACGDBlizn7v3Lmj0Wi4rQGAh4dHY2NjNw4KujE2Xrt27dy5cydPnuQGluLi4l27dnV2dvbq1QsArl+/rre+sXYAEIvFHR0d3ONbt249vtaurH/v3r358+enp6d///33J06c8PPzM71+r169RCLRnj17VP9i7969ACCTyezt7bmOZRB7e3tz9svd8aD1dO3aNdP1mKDLzsrKysLDw7W3fgYGBtra2h4/ftzBwWH06NGrVq26fft2Q0NDXl5efn4+ABhrB4A+ffocOHDg4cOHP//8M/cGmaZL6zc3N6vV6oEDBwLA+vXrW1pa6urqtF3N0dHx4sWLHR0dLS0t3EhgZ2cXFxf36aefNjY23r59+6233vriiy8AwN7efurUqStWrLh06dL9+/ePHz/Oda+u7lcqlYaEhGzcuPHevXsnTpw4cuTIY4/XGF12VlpaGh0drdsyatSo0tJSAHjvvffEYnFCQsKUKVPEYrHWjbH22bNnV1ZWRkdHr169Oi0t7bEDoMH1L1++zH38+/TTTysqKrjHjY2N3t7eOTk5L7/8cnJysoODw4wZMz7//POKigpuU1lZWYcPHx4+fPikSZOOHv1nAPnixYs1Gs3EiRMnTpzY0dGRlZXFtc+bNy8qKkqpVMbGxh44cGDt2rUmijSx34KCgqqqqtGjR3/55ZcpKSndvkPgd/cWbN68OSMjg43fYpghLy/PxcVly5Yt2hYWrl39p0HO8EHO8EHO8EHO8EHO8EHO8EHO8GHgGjF3lZp4ekhNTdV9+rvrIMzc+w0AGRkZc+bM+d0t02jRu/ebwf/FcFAOPPEUQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7wQc7w0c35iJ9CvvrqK73Z9bZv366dAxUAZsyY4eXlJXhd/MPOfwZnzZq1du1aqVTKPdVoNNoJ4Do6OlxdXRsaGtjIQmNnbOQm8W79F21tbdrHYrFYoVCwIQxY6mednZ3e3t4GJ/cEgEOHDjGThcZOPxOLxdnZ2Y/mUwOAt7d3ZGSk8CVZCHacAYBCoWhra9NrlEgkSqXS9OSmuGBnbOTw9/fX/azIUVlZ+fzzz1ulHkvAVD8DAKVSqfdZw9/fnyVhwJ4zhUKhlwM/c+ZMK9ZjCVgbGwFg8ODBp06d0h7XhQsXgoKCrFsSv7DWzwBAqVTa2NgAgEgkGjp0KGPCgElnmZmZarUaAGxsbKZPn27tcviHQWc+Pj6RkZEikaizszMtLc3a5fAPg84AICcnR6PRjBo1ysfHx9q1WACNGVDWuDDoZYcbowu/xSxdutRy5fJOYWFhWlqag4ODtQsxl/Xr15u5ZhecjR8/vlvFWIchQ4bg+rVsz549Zq7J5vkMAHAJ6xLMOmMYcoYPcoYPazqzdH77Z599ps3HszRPkuveVXhzdu7cOblcro1jXrRoUWhoqOmXcPntc+bM0WsPDQ2Vy+VcuGx+fj6X3YiIpqamuXPnRkdHjx07dtmyZd0LkDUBn/3M09OTi6vmIjefZFOFhYVVVVWbNm2qr683Jx75qeL9999vb2/fsWPHn//85/379/N+RYJPZ+7u7m5ublVVVfv27dNNAO5qfjuHSCTy8fGJjY2tra3lWozlqxvLmdfS1tY2ffp0zn1WVtbu3bt1ly5fvvz99983sX1jee8Gc91bW1srKirmzZvn4eHh5+f38ssvl5WVmXO85sPz+Wz8+PG7d+8uLy9PSEh4wk1pNJra2tpdu3bFx8dzLcby1U3nzHd2di5cuLB///65ubkAEBAQ8Msvv+iuUF1dHRgYaGL7xvLeDea619XVAYCvry/3NCAgoLq6+gnfCj14vo94woQJ06dPt7W1HTp06JNsR/sbSmRkZFJSEvfYYL76Y3PmP/roIwB4++23uacBAQFnz54FgKSkpODg4I8//rimpuall14ytn0wkvduLNf9wYMHUqlUJBIlJyeHhIS8+uqr9+/ff5K34lF4dubp6SmTyYKDg5/wPqfCwsJhw4Y1NzcXFxenpaUVFxc7Ojoay1cH4znzBw8e7OzsnDx5slj8zxElICBg165dZ86cCQoKOn/+/N27d69evcr1M2O58Qbz3o3lujs4OLS2tmo0mpKSEgBQqVS8X/Pk/7P+559//uabb/5uH13Me9fi4uIyffr027dvq1QqY/nqJnLmASAkJKSkpKS8vHz//v1cS0BAwNWrV/fu3RsXFxcaGvrtt9/27NnTxcXlsbnxennvxnLd+/XrZ2NjU1NTwz09f/78gAEDzD9kcxDi+1lX89613L9/f9OmTRqNxtfX11i+uomceQDw8PDo2bPnu++++6c//Yl7W729vdvb2w8ePDh69Oi4uLjvvvuO62Smc+MfxViuu52dXUJCwtKlSxsbG2tqatatWzdp0qRuvW1GEcJZl/LbuZdMnz5dLpfHxsbu3LlzzZo1Pj4+JvLVjeXMa4mOjp4wYcKCBQu4Xffv39/FxcXV1TU8PLy+vp5zZjo33iDGct3z8/MdHR2TkpJeffXVF198cfLkyfy+n2bdd0X58ALwaN67Meh6Iz7IGT7IGT7IGT7IGT7IGT7IGT66cL1Rez2bsARVVVUxMTHmrGmWM5lMphdF/vRz9OjRoKAgT09PaxdiLjExMWYmoDP4/zMOyhQnniLIGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7YyRT/9ddfuTgmLdeuXdPOFQYA3t7eevOMIYWd/wwmJibu2rXL2FJbW9uGhgZEf/s0ATtj47Rp04xNGikWi1944QU2hAFLzqZMmWIi6T0nJ0fIYiwKO86cnZ2TkpIMapNIJMnJycKXZCHYcQYAWVlZ2glZtdja2qakpDg5OVmlJEvAlLMJEyY4OjrqNarV6qysLKvUYyGYciaVSlNTU+3s7HQbnZyccCW3PRamnAFAZmZmW1ub9qlEIpk2bZqeReyw8/2MQ61We3l56c4u/sMPP2gnv2cD1vqZjY1NVlaWtmP16tXLzEmkEMGaMwBQKBTc8CiRSLSZ8CzB2tgIABqNxs/P78qVKwBw7NixsLAwa1fEMwz2M5FIxF318PX1ZU8YmHld/8iRI8uWLbN0KTzS3NwMAE5OTmlpadaupQtERESYE7lolrMrV65s3bp13LhxT1yVcDg7O7u5uXHyUFBVVWXmml34/UwvV+wp59ChQ9HR0dauoguYPw0tg+czDlzCugSzzhiGnOGDnOHDms5083QtAauZ4nzed3X69OmlS5eeOXNGLBaHhIQsWrTI39+/G9sJDQ1ta2sTiUQeHh4jR47Mz893c3PjsU5L09TUtGTJkmPHjkml0gkTJsyZM0ebJsoLfG4rNzd35MiR+/bt27Vrl0wme5Kv4ZQpbgLenLW2tv7222/x8fGOjo6urq4FBQWrV6/mFpnIFK+pqUlLSxsxYsRrr72mDYbkoExxY/DmTCqVRkRE/OEPf9ixY4f2AB5LWVnZxx9/vHv37paWli+++EJ3EWWKG4PP89mqVau2bdtWXFy8ZMkSuVy+YMGC4OBg0y+ZPHnyM888AwDp6el/+ctftO2UKW4CPp1JpVKFQqFQKO7evbtixYrXX3+9oqLC1tbULry8vLgH7u7uumMaZYqbwCKf9Z2cnF555ZWbN29yv/GbyBS/ffu29oH2+LVQprhBeHP2008/JSQkVFZWtre3Nzc3b9iwwcfHp3fv3mAyU3zbtm319fWNjY1btmwJDw/X2yZlihuEN2fh4eHZ2dkFBQURERFJSUnV1dVr1qzhRhWDmeIA0NHRkZCQ8Nprr40bN87V1XXWrFnarVGmuAkoU/xpgTLFWYac4YOc4YOc4YOc4YOc4YOc4aML1xv1fsIg+KWhoYG7HvZYuuBs3rx53a2HMAszr0wy+B8LDsoUJ54iyBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+yBk+2PnPYHZ2tm58R3V1tZeXlzZ6VSKRlJSU9O3b10rV8Qmf8zdal2effXbjxo26LbrzoQ4cOJANYcDS2JiZmWksB14ikcyYMUPYciwIO2MjAAwbNqyqqko716AWkUhUU1PDTaLLAOz0MwBQKpWPzmQvEonCwsKYEQaMOZs2bdqjnUwsFiuVSqvUYyGYctanT5+YmJhHQ1dTU1OtUo+FYMoZAHBJnlrEYnFcXJx2dnE2YM3Z1KlT9U5pehYZgDVnbm5uCQkJ2kn9bWxseJ922+qw5gwAsrOz1Wo1ANja2iYnJ7u6ulq7Ip5h0NnEiRO5/AK1Wp2dnW3tcviHQWc9evSYMmUKADg4OCQmJlq7HP4x63pjXV3d4cOHLV0Kj8hkMgAICwvbsWOHtWvpAjKZLCIi4vHracyA99Q1wiCpqanm6OjCdX1cORafffbZ7NmzH/1+/dRCmeIwa9YsRMK6BLPOTOeuoYZZZwxDzvDBsjPKFH88XBY4ALi4uMjl8nnz5nFxcN3bDmWKG4PnflZYWKhSqUpKSmQy2Ztvvvkk26FMcWNYZGz08PBQKBRXrly5e/cuANy4cWPu3LlRUVFxcXHvvvvugwcPwGS2NwdlihvDIs5u3Lixbt264OBg7vZCLhu6rKxs48aNKpWKyw43ke3NQZnixuD5SwyXBe7m5hYaGrpixQoAePDgwaFDh7Zv3+7m5ubm5paTk/PVV1/l5uaayPYGyhQ3Cc/OuCxw3RZu0OBCj7kHXES1iWxvoExxk1j8s37v3r1FIpE28/v69evc3RnGsr11X0uZ4gaxuDOpVDpq1Kg1a9a0tLRcvXq1sLBw3LhxYDzbWxfKFDeIEN+pFy9erFarx40bp1Qqw8PDtSctg9neHJQpbgLKFH9aoExxliFn+CBn+CBn+CBn+CBn+CBn+CBn+OjCNWLuqjZhOcz8b6NZ10HQ3fsNABkZGXPmzDHrVuqnBjPv/WZq3gJdKAeeeIogZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/ggZ/hg5/9nixcv/u2337RP//a3v0VGRmrnvgSAgoICHx8fa5TGM+w4y8vLW7ZsGTchph5qtdrLy+vq1avGAqxxwc7YqFAoAKDdEDY2Nkqlkg1hwFI/AwB/f//Lly8bXFRZWfn8888LXI+FYKefAUBOTo7BsdHf358ZYcCYM4VC0d7ertcokUhmzpxplXosBFNjIwAMHjz41KlTegd14cKFoKAga5XEO0z1MwBQKpW6sWcikWjo0KEsCQP2nGVmZnJBxxw2NjbaqfqZgTVnPj4+ERER2tn01Wq1mRMSIYI1ZwCQk5PDfRUTi8WjRo3q27evtSviGQadpaenc85EIlFOTo61y+EfBp15eHjEx8eLRCKRSMSFizMGg84AIDs7W6PRJCYmavNcmMKcEGu+sPaxWpCioiLB3kahA4EVc4bKI7wF2NG2L1VJMwbZSYWIqF6UsVOAvWgR2pk8wjs+neecIoOEj/NzdpMKsCMQ3Bmb5zMAEEyY8DDrjGHIGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7IGT7YdNb2sCNtYOGPxZd0G1fnH5o/ucRaJfEIm87WLvkpUN4zdnKgbuOsJRE1p27t+/aSsVdhgUFnTbceblld+fKfwvXa7aQ22fNDv15yFPvNsQw627v1ot9Aj8DBPR9dlJgd/Ov5xguV14WvikcYdHbix7pho/sZXNTDwXZQmFflwXqBS+IXBp1dr2vp4+tsbGkfP5eG2hYh6+EdBp3da2l3dLEzttTZTXqvuVXIeniHQWeOzpJ7zW3GlrbcaXVyxX1bOIPOvGTOJka/hl+bvWRGR04UMOhsWGy/kwfqDC5qfdBx5vi1oaNw/8OaQWdjU4N+OXu7+tStRxeVbzjnN9B9wJBewlfFIww6c/HokfbGkK+XHNVrb2tVf/PR3195e6RVquIRBp0BwCtvh19S3fxh2+8uU31VcMQ/xHPM1EBjr8KC0P/zFAapve2Wc/rT77zxYbRViuEdNvsZ25AzfJAzfJAzfAj9GWTT8pN7t1wUeKeMIagz9qbl40hNfV4mkwm2O9bmtv1PgM5n+CBn+CBn+Ph/X9F+7Ypdo9kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent with Autograd and Backpropagation\n"
      ],
      "metadata": {
        "id": "SoyLA9sAkTu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First, we will do this by\n",
        "1. <B>Prediction:</B> Manually\n",
        "2. <B>Gradient Calculation:</B> Manually\n",
        "3. <B>Loss Computation:</B> Manually\n",
        "4. <B>Parameters Update:</B> Manually\n"
      ],
      "metadata": {
        "id": "h6-wxVQfdBqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import numpy as np\n",
        "# Manually finding the f = w*x using Deep Learning\n",
        "\n",
        "# Here, f = 2*x  i.e. w = 2\n",
        "X = np.array([1, 2, 3, 4], dtype = np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype = np.float32)\n",
        "w = 0.0 # Initializing the value of w = 0.0\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted): # Finds the loss value\n",
        "  return ((y_predicted - y)**2).mean() # Mean Squared Error\n",
        "\n",
        "# Gradient\n",
        "# MSE = ((w*x - y)**2)/N  i.e.  δϵ/δw = 2(w*x - y)x/N\n",
        "def gradient(x, y, y_predicted): # Compute the gradient\n",
        "  return np.dot(2*x, y_predicted - y).mean()\n",
        "\n",
        "print(f\"Prediction before training: f(7) = {forward(7):.4f}\")\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.009 # Fixing the Learning Rate(η)\n",
        "n_iters = 15 # Number of iterations, change these 2 and check the outputs\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # Prediction = forward pass\n",
        "  y_predicted = forward(X)\n",
        "\n",
        "  # Loss\n",
        "  l = loss(Y, y_predicted)\n",
        "\n",
        "  # Gradients\n",
        "  dw = gradient(X, Y, y_predicted)\n",
        "\n",
        "  # Update Weights\n",
        "  w -= learning_rate*dw\n",
        "  if epoch%1 == 0:\n",
        "    print(f\"Epoch {epoch+1}: Weight(w) = {w:.3f}, Loss(MSE) = {l:.8f}\")\n",
        "\n",
        "print(f\"Prediction after training: f(7) = {forward(7):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIRxKpGKTq1T",
        "outputId": "d0b9291c-040b-4bc2-fa2b-e60cebe1698b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(7) = 0.0000\n",
            "Epoch 1: Weight(w) = 1.080, Loss(MSE) = 30.00000000\n",
            "Epoch 2: Weight(w) = 1.577, Loss(MSE) = 6.34799910\n",
            "Epoch 3: Weight(w) = 1.805, Loss(MSE) = 1.34323680\n",
            "Epoch 4: Weight(w) = 1.910, Loss(MSE) = 0.28422886\n",
            "Epoch 5: Weight(w) = 1.959, Loss(MSE) = 0.06014294\n",
            "Epoch 6: Weight(w) = 1.981, Loss(MSE) = 0.01272627\n",
            "Epoch 7: Weight(w) = 1.991, Loss(MSE) = 0.00269286\n",
            "Epoch 8: Weight(w) = 1.996, Loss(MSE) = 0.00056981\n",
            "Epoch 9: Weight(w) = 1.998, Loss(MSE) = 0.00012057\n",
            "Epoch 10: Weight(w) = 1.999, Loss(MSE) = 0.00002551\n",
            "Epoch 11: Weight(w) = 2.000, Loss(MSE) = 0.00000540\n",
            "Epoch 12: Weight(w) = 2.000, Loss(MSE) = 0.00000114\n",
            "Epoch 13: Weight(w) = 2.000, Loss(MSE) = 0.00000024\n",
            "Epoch 14: Weight(w) = 2.000, Loss(MSE) = 0.00000005\n",
            "Epoch 15: Weight(w) = 2.000, Loss(MSE) = 0.00000001\n",
            "Prediction after training: f(7) = 13.9999\n",
            "CPU times: user 6.47 ms, sys: 0 ns, total: 6.47 ms\n",
            "Wall time: 6.89 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second, we will do this by\n",
        "1. <B>Prediction:</B> Manually\n",
        "2. <B>Gradient Calculation:</B> Autograd\n",
        "3. <B>Loss Computation:</B> Manually\n",
        "4. <B>Parameters Update:</B> Manually"
      ],
      "metadata": {
        "id": "vNlmpOf6djoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "# Manually finding the f = w*x using Deep Learning\n",
        "\n",
        "# Here, f = 2*x  i.e. w = 2\n",
        "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype = torch.float32)\n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True) # Initializing the value of w = 0.0\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted): # Finds the loss value\n",
        "  return ((y_predicted - y)**2).mean() # Mean Squared Error\n",
        "\n",
        "# Gradient (not using NumPy array anymore i.e. no need to calculate the gradient)\n",
        "# MSE = ((w*x - y)**2)/N  i.e.  δϵ/δw = 2(w*x - y)x/N\n",
        "# def gradient(x, y, y_predicted): # Compute the gradient\n",
        "#   return np.dot(2*x, y_predicted - y).mean()\n",
        "\n",
        "print(f\"Prediction before training: f(7) = {forward(7):.4f}\")\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01 # Fixing the Learning Rate(η)\n",
        "n_iters = 25 # Number of iterations, change these 2 and check the outputs\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # Prediction = forward pass\n",
        "  y_predicted = forward(X)\n",
        "\n",
        "  # Loss\n",
        "  l = loss(Y, y_predicted)\n",
        "\n",
        "  # Gradients = backward pass\n",
        "  l.backward() # δϵ/δw\n",
        "\n",
        "  # Update Weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate*w.grad\n",
        "  w.grad.zero_() # Resets the gradient to zero, optimization step\n",
        "  if epoch%1 == 0:\n",
        "    print(f\"Epoch {epoch+1}: Weight(w) = {w:.3f}, Loss(MSE) = {l:.8f}\")\n",
        "\n",
        "print(f\"Prediction after training: f(7) = {forward(7):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCxW1VarYlnY",
        "outputId": "0e6728ce-3f65-4be0-cb24-1e8ce9a4893a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(7) = 0.0000\n",
            "Epoch 1: Weight(w) = 0.300, Loss(MSE) = 30.00000000\n",
            "Epoch 2: Weight(w) = 0.555, Loss(MSE) = 21.67499924\n",
            "Epoch 3: Weight(w) = 0.772, Loss(MSE) = 15.66018772\n",
            "Epoch 4: Weight(w) = 0.956, Loss(MSE) = 11.31448650\n",
            "Epoch 5: Weight(w) = 1.113, Loss(MSE) = 8.17471695\n",
            "Epoch 6: Weight(w) = 1.246, Loss(MSE) = 5.90623236\n",
            "Epoch 7: Weight(w) = 1.359, Loss(MSE) = 4.26725292\n",
            "Epoch 8: Weight(w) = 1.455, Loss(MSE) = 3.08308983\n",
            "Epoch 9: Weight(w) = 1.537, Loss(MSE) = 2.22753215\n",
            "Epoch 10: Weight(w) = 1.606, Loss(MSE) = 1.60939169\n",
            "Epoch 11: Weight(w) = 1.665, Loss(MSE) = 1.16278565\n",
            "Epoch 12: Weight(w) = 1.716, Loss(MSE) = 0.84011245\n",
            "Epoch 13: Weight(w) = 1.758, Loss(MSE) = 0.60698116\n",
            "Epoch 14: Weight(w) = 1.794, Loss(MSE) = 0.43854395\n",
            "Epoch 15: Weight(w) = 1.825, Loss(MSE) = 0.31684780\n",
            "Epoch 16: Weight(w) = 1.851, Loss(MSE) = 0.22892261\n",
            "Epoch 17: Weight(w) = 1.874, Loss(MSE) = 0.16539653\n",
            "Epoch 18: Weight(w) = 1.893, Loss(MSE) = 0.11949898\n",
            "Epoch 19: Weight(w) = 1.909, Loss(MSE) = 0.08633806\n",
            "Epoch 20: Weight(w) = 1.922, Loss(MSE) = 0.06237914\n",
            "Epoch 21: Weight(w) = 1.934, Loss(MSE) = 0.04506890\n",
            "Epoch 22: Weight(w) = 1.944, Loss(MSE) = 0.03256231\n",
            "Epoch 23: Weight(w) = 1.952, Loss(MSE) = 0.02352631\n",
            "Epoch 24: Weight(w) = 1.960, Loss(MSE) = 0.01699772\n",
            "Epoch 25: Weight(w) = 1.966, Loss(MSE) = 0.01228084\n",
            "Prediction after training: f(7) = 13.7592\n",
            "CPU times: user 23.4 ms, sys: 1 ms, total: 24.4 ms\n",
            "Wall time: 24.7 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third, we will do this by\n",
        "1. <B>Prediction:</B> Manually\n",
        "2. <B>Gradient Calculation:</B> Autograd\n",
        "3. <B>Loss Computation:</B> PyTorch Loss\n",
        "4. <B>Parameters Update:</B> PyTorch Optimizer"
      ],
      "metadata": {
        "id": "Li5AkfHHdkib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Steps\n",
        "# 1) Design the model (input size, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#   - Forward pass: compute prediction\n",
        "#   - Backward pass: compute gradients\n",
        "#   - Update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Manually finding the f = w*x using Deep Learning\n",
        "\n",
        "# Here, f = 2*x  i.e. w = 2\n",
        "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype = torch.float32)\n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True) # Initializing the value of w = 0.0\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# Loss = MSE (Since not manually doing)\n",
        "# def loss(y, y_predicted): # Finds the loss value\n",
        "#   return ((y_predicted - y)**2).mean() # Mean Squared Error\n",
        "\n",
        "# Gradient (not using NumPy array anymore i.e. no need to calculate the gradient)\n",
        "# MSE = ((w*x - y)**2)/N  i.e.  δϵ/δw = 2(w*x - y)x/N\n",
        "# def gradient(x, y, y_predicted): # Compute the gradient\n",
        "#   return np.dot(2*x, y_predicted - y).mean()\n",
        "\n",
        "print(f\"Prediction before training: f(7) = {forward(7):.4f}\")\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01 # Fixing the Learning Rate(η)\n",
        "n_iters = 25 # Number of iterations, change these 2 and check the outputs\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # Prediction = forward pass\n",
        "  y_predicted = forward(X)\n",
        "\n",
        "  # Loss\n",
        "  l = loss(Y, y_predicted)\n",
        "\n",
        "  # Gradients = backward pass\n",
        "  l.backward() # δϵ/δw\n",
        "\n",
        "  # Update Weights\n",
        "  # with torch.no_grad():\n",
        "    # w -= learning_rate*w.grad\n",
        "  optimizer.step()\n",
        "  # w.grad.zero_() # Resets the gradient to zero, optimization step\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch%1 == 0:\n",
        "    print(f\"Epoch {epoch+1}: Weight(w) = {w:.3f}, Loss(MSE) = {l:.8f}\")\n",
        "\n",
        "print(f\"Prediction after training: f(7) = {forward(7):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqxgkTGddk4L",
        "outputId": "0228770e-9670-4178-d7f4-439b4209a4c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(7) = 0.0000\n",
            "Epoch 1: Weight(w) = 0.300, Loss(MSE) = 30.00000000\n",
            "Epoch 2: Weight(w) = 0.555, Loss(MSE) = 21.67499924\n",
            "Epoch 3: Weight(w) = 0.772, Loss(MSE) = 15.66018772\n",
            "Epoch 4: Weight(w) = 0.956, Loss(MSE) = 11.31448650\n",
            "Epoch 5: Weight(w) = 1.113, Loss(MSE) = 8.17471695\n",
            "Epoch 6: Weight(w) = 1.246, Loss(MSE) = 5.90623236\n",
            "Epoch 7: Weight(w) = 1.359, Loss(MSE) = 4.26725292\n",
            "Epoch 8: Weight(w) = 1.455, Loss(MSE) = 3.08308983\n",
            "Epoch 9: Weight(w) = 1.537, Loss(MSE) = 2.22753215\n",
            "Epoch 10: Weight(w) = 1.606, Loss(MSE) = 1.60939169\n",
            "Epoch 11: Weight(w) = 1.665, Loss(MSE) = 1.16278565\n",
            "Epoch 12: Weight(w) = 1.716, Loss(MSE) = 0.84011245\n",
            "Epoch 13: Weight(w) = 1.758, Loss(MSE) = 0.60698116\n",
            "Epoch 14: Weight(w) = 1.794, Loss(MSE) = 0.43854395\n",
            "Epoch 15: Weight(w) = 1.825, Loss(MSE) = 0.31684780\n",
            "Epoch 16: Weight(w) = 1.851, Loss(MSE) = 0.22892261\n",
            "Epoch 17: Weight(w) = 1.874, Loss(MSE) = 0.16539653\n",
            "Epoch 18: Weight(w) = 1.893, Loss(MSE) = 0.11949898\n",
            "Epoch 19: Weight(w) = 1.909, Loss(MSE) = 0.08633806\n",
            "Epoch 20: Weight(w) = 1.922, Loss(MSE) = 0.06237914\n",
            "Epoch 21: Weight(w) = 1.934, Loss(MSE) = 0.04506890\n",
            "Epoch 22: Weight(w) = 1.944, Loss(MSE) = 0.03256231\n",
            "Epoch 23: Weight(w) = 1.952, Loss(MSE) = 0.02352631\n",
            "Epoch 24: Weight(w) = 1.960, Loss(MSE) = 0.01699772\n",
            "Epoch 25: Weight(w) = 1.966, Loss(MSE) = 0.01228084\n",
            "Prediction after training: f(7) = 13.7592\n",
            "CPU times: user 2.85 s, sys: 778 ms, total: 3.63 s\n",
            "Wall time: 6.72 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forth, we will do this by\n",
        "1. <B>Prediction:</B> PyTorch Model\n",
        "2. <B>Gradient Calculation:</B> Autograd\n",
        "3. <B>Loss Computation:</B> PyTorch Loss\n",
        "4. <B>Parameters Update:</B> PyTorch Optimizer"
      ],
      "metadata": {
        "id": "-Qso9dmId-Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Steps\n",
        "# 1) Design the model (input size, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#   - Forward pass: compute prediction\n",
        "#   - Backward pass: compute gradients\n",
        "#   - Update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Finding the f = w*x using Deep Learning\n",
        "\n",
        "# Here, f = 2*x  i.e. w = 2\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
        "# w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True) # Initializing the value of w = 0.0\n",
        "X_test = torch.tensor([7], dtype = torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(\"No. of samples:\", n_samples, \"No. of features:\", n_features)\n",
        "\n",
        "input_size, output_size = n_features, n_features\n",
        "model = nn.Linear(input_size, output_size) # Linear regression model\n",
        "\n",
        "# Model prediction\n",
        "# def forward(x):\n",
        "#   return w*x\n",
        "\n",
        "# Loss = MSE (Since not manually doing)\n",
        "# def loss(y, y_predicted): # Finds the loss value\n",
        "#   return ((y_predicted - y)**2).mean() # Mean Squared Error\n",
        "\n",
        "# Gradient (not using NumPy array anymore i.e. no need to calculate the gradient)\n",
        "# MSE = ((w*x - y)**2)/N  i.e.  δϵ/δw = 2(w*x - y)x/N\n",
        "# def gradient(x, y, y_predicted): # Compute the gradient\n",
        "#   return np.dot(2*x, y_predicted - y).mean()\n",
        "\n",
        "print(f\"Prediction before training: f(7) = {model(X_test).item():.4f}\")\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01 # Fixing the Learning Rate(η)\n",
        "n_iters = 25 # Number of iterations, change these 2 and check the outputs\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "#  Prediction = forward pass\n",
        "  y_predicted = model(X)\n",
        "\n",
        "  # Loss\n",
        "  l = loss(Y, y_predicted)\n",
        "\n",
        "  # Gradients = backward pass\n",
        "  l.backward() # δϵ/δw\n",
        "\n",
        "  # Update Weights\n",
        "#   with torch.no_grad():\n",
        "#     w -= learning_rate*w.grad\n",
        "  optimizer.step()\n",
        "#   w.grad.zero_() # Resets the gradient to zero, optimization step\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch%1 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f\"Epoch {epoch+1}: Weight(w) = {w[0][0].item():.3f}, Loss(MSE) = {l:.8f}\")\n",
        "\n",
        "print(f\"Prediction after training: f(7) = {model(X_test).item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIBjcQehd-uQ",
        "outputId": "14347250-37b8-4693-bee5-144ff87f7d7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of samples: 4 No. of features: 1\n",
            "Prediction before training: f(7) = 3.0258\n",
            "Epoch 1: Weight(w) = 0.765, Loss(MSE) = 21.13142776\n",
            "Epoch 2: Weight(w) = 0.975, Loss(MSE) = 14.66282082\n",
            "Epoch 3: Weight(w) = 1.149, Loss(MSE) = 10.17439079\n",
            "Epoch 4: Weight(w) = 1.294, Loss(MSE) = 7.05996275\n",
            "Epoch 5: Weight(w) = 1.415, Loss(MSE) = 4.89892483\n",
            "Epoch 6: Weight(w) = 1.516, Loss(MSE) = 3.39942646\n",
            "Epoch 7: Weight(w) = 1.600, Loss(MSE) = 2.35895443\n",
            "Epoch 8: Weight(w) = 1.670, Loss(MSE) = 1.63699222\n",
            "Epoch 9: Weight(w) = 1.728, Loss(MSE) = 1.13603699\n",
            "Epoch 10: Weight(w) = 1.777, Loss(MSE) = 0.78843331\n",
            "Epoch 11: Weight(w) = 1.817, Loss(MSE) = 0.54723787\n",
            "Epoch 12: Weight(w) = 1.851, Loss(MSE) = 0.37987661\n",
            "Epoch 13: Weight(w) = 1.879, Loss(MSE) = 0.26374704\n",
            "Epoch 14: Weight(w) = 1.902, Loss(MSE) = 0.18316615\n",
            "Epoch 15: Weight(w) = 1.922, Loss(MSE) = 0.12725203\n",
            "Epoch 16: Weight(w) = 1.938, Loss(MSE) = 0.08845331\n",
            "Epoch 17: Weight(w) = 1.951, Loss(MSE) = 0.06153070\n",
            "Epoch 18: Weight(w) = 1.963, Loss(MSE) = 0.04284897\n",
            "Epoch 19: Weight(w) = 1.972, Loss(MSE) = 0.02988505\n",
            "Epoch 20: Weight(w) = 1.980, Loss(MSE) = 0.02088877\n",
            "Epoch 21: Weight(w) = 1.986, Loss(MSE) = 0.01464554\n",
            "Epoch 22: Weight(w) = 1.991, Loss(MSE) = 0.01031257\n",
            "Epoch 23: Weight(w) = 1.996, Loss(MSE) = 0.00730510\n",
            "Epoch 24: Weight(w) = 2.000, Loss(MSE) = 0.00521743\n",
            "Epoch 25: Weight(w) = 2.003, Loss(MSE) = 0.00376792\n",
            "Prediction after training: f(7) = 13.9598\n",
            "CPU times: user 30.3 ms, sys: 7.55 ms, total: 37.8 ms\n",
            "Wall time: 35.9 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Last, we want a custom model with PyTorch with the followings\n",
        "1. <B>Prediction:</B> PyTorch Model\n",
        "2. <B>Gradient Calculation:</B> Autograd\n",
        "3. <B>Loss Computation:</B> PyTorch Loss\n",
        "4. <B>Parameters Update:</B> PyTorch Optimizer"
      ],
      "metadata": {
        "id": "hX5mpRLMAdFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Steps\n",
        "# 1) Design the model (input size, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#   - Forward pass: compute prediction\n",
        "#   - Backward pass: compute gradients\n",
        "#   - Update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
        "X_test = torch.tensor([7], dtype = torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(\"No. of samples:\", n_samples, \"No. of features:\", n_features)\n",
        "\n",
        "input_size, output_size = n_features, n_features\n",
        "# model = nn.Linear(input_size, output_size) # Linear regression model\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    # Define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size) # Custom Linear Regression Model\n",
        "\n",
        "print(f\"Prediction before training: f(7) = {model(X_test).item():.4f}\")\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01 # Fixing the Learning Rate(η)\n",
        "n_iters = 25 # Number of iterations, change these 2 and check the outputs\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "#  Prediction = forward pass\n",
        "  y_predicted = model(X)\n",
        "\n",
        "  # Loss\n",
        "  l = loss(Y, y_predicted)\n",
        "\n",
        "  # Gradients = backward pass\n",
        "  l.backward() # δϵ/δw\n",
        "\n",
        "  # Update Weights\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad() # Resets the gradient to zero, optimization step\n",
        "\n",
        "  if epoch%1 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f\"Epoch {epoch+1}: Weight(w) = {w[0][0].item():.3f}, Loss(MSE) = {l:.8f}\")\n",
        "\n",
        "print(f\"Prediction after training: f(7) = {model(X_test).item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDHvaQO5AJ2g",
        "outputId": "f52e00b5-b20f-4fc1-beb9-cc88050a3d48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of samples: 4 No. of features: 1\n",
            "Prediction before training: f(7) = -0.6305\n",
            "Epoch 1: Weight(w) = 0.378, Loss(MSE) = 38.50626755\n",
            "Epoch 2: Weight(w) = 0.661, Loss(MSE) = 26.72090912\n",
            "Epoch 3: Weight(w) = 0.896, Loss(MSE) = 18.54328156\n",
            "Epoch 4: Weight(w) = 1.092, Loss(MSE) = 12.86898804\n",
            "Epoch 5: Weight(w) = 1.255, Loss(MSE) = 8.93169975\n",
            "Epoch 6: Weight(w) = 1.391, Loss(MSE) = 6.19968796\n",
            "Epoch 7: Weight(w) = 1.504, Loss(MSE) = 4.30398846\n",
            "Epoch 8: Weight(w) = 1.599, Loss(MSE) = 2.98859024\n",
            "Epoch 9: Weight(w) = 1.677, Loss(MSE) = 2.07585001\n",
            "Epoch 10: Weight(w) = 1.743, Loss(MSE) = 1.44250655\n",
            "Epoch 11: Weight(w) = 1.797, Loss(MSE) = 1.00303042\n",
            "Epoch 12: Weight(w) = 1.842, Loss(MSE) = 0.69807494\n",
            "Epoch 13: Weight(w) = 1.880, Loss(MSE) = 0.48645988\n",
            "Epoch 14: Weight(w) = 1.911, Loss(MSE) = 0.33961198\n",
            "Epoch 15: Weight(w) = 1.937, Loss(MSE) = 0.23770514\n",
            "Epoch 16: Weight(w) = 1.959, Loss(MSE) = 0.16698191\n",
            "Epoch 17: Weight(w) = 1.977, Loss(MSE) = 0.11789630\n",
            "Epoch 18: Weight(w) = 1.992, Loss(MSE) = 0.08382455\n",
            "Epoch 19: Weight(w) = 2.005, Loss(MSE) = 0.06017089\n",
            "Epoch 20: Weight(w) = 2.015, Loss(MSE) = 0.04374621\n",
            "Epoch 21: Weight(w) = 2.023, Loss(MSE) = 0.03233738\n",
            "Epoch 22: Weight(w) = 2.031, Loss(MSE) = 0.02440933\n",
            "Epoch 23: Weight(w) = 2.036, Loss(MSE) = 0.01889637\n",
            "Epoch 24: Weight(w) = 2.041, Loss(MSE) = 0.01505937\n",
            "Epoch 25: Weight(w) = 2.045, Loss(MSE) = 0.01238525\n",
            "Prediction after training: f(7) = 14.1149\n",
            "CPU times: user 41.8 ms, sys: 4.67 ms, total: 46.5 ms\n",
            "Wall time: 44.1 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression using PyTorch"
      ],
      "metadata": {
        "id": "S-L8ABt9B_w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Steps\n",
        "# 1) Design the model (input size, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#   - Forward pass: compute prediction\n",
        "#   - Backward pass: compute gradients\n",
        "#   - Update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) Prepare the data\n",
        "X_numpy, Y_numpy = datasets.make_regression(n_samples = 200, n_features = 1, noise = 40, random_state = 1) # Important Parameters\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
        "Y = Y.view(Y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape # Shape of X\n",
        "\n",
        "# 1) Model\n",
        "input_size, output_size = n_features, 1\n",
        "\n",
        "model = nn.Linear(input_size, output_size) # Linear Regression Model\n",
        "\n",
        "# 2) Loss and optimizer\n",
        "learning_rate = 0.1\n",
        "criterion = nn.MSELoss() # Mean Squared Error\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # Stochastic Gradient Descent\n",
        "\n",
        "\n",
        "# 3) Training loop\n",
        "num_epochs = 200 # Number of iterations, check outputs with combination of this and learning rate\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # Forward pass and loss\n",
        "  y_predicted = model(X)\n",
        "  loss = criterion(y_predicted, Y)\n",
        "\n",
        "  # Backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # Update weights\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad() # Resets the gradient to zero, optimization step\n",
        "\n",
        "  if (epoch+1)%10 == 0:\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "# Plot\n",
        "predicted = model(X).detach().numpy() # Detach the tensor from the computational graph\n",
        "plt.plot(X_numpy, Y_numpy, \"ro\")\n",
        "plt.plot(X_numpy, predicted, \"b\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "kYuINHHtCAEq",
        "outputId": "f4fd3bdb-a871-4f3b-8d49-ceb97469e88d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Loss = 1805.3328\n",
            "Epoch 20: Loss = 1559.9766\n",
            "Epoch 30: Loss = 1552.5233\n",
            "Epoch 40: Loss = 1552.2848\n",
            "Epoch 50: Loss = 1552.2770\n",
            "Epoch 60: Loss = 1552.2769\n",
            "Epoch 70: Loss = 1552.2767\n",
            "Epoch 80: Loss = 1552.2769\n",
            "Epoch 90: Loss = 1552.2770\n",
            "Epoch 100: Loss = 1552.2770\n",
            "Epoch 110: Loss = 1552.2770\n",
            "Epoch 120: Loss = 1552.2770\n",
            "Epoch 130: Loss = 1552.2770\n",
            "Epoch 140: Loss = 1552.2770\n",
            "Epoch 150: Loss = 1552.2770\n",
            "Epoch 160: Loss = 1552.2770\n",
            "Epoch 170: Loss = 1552.2770\n",
            "Epoch 180: Loss = 1552.2770\n",
            "Epoch 190: Loss = 1552.2770\n",
            "Epoch 200: Loss = 1552.2770\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGdCAYAAADnrPLBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWC0lEQVR4nO3de3wTVfo/8E9aoBShBUovQIsoXvDCF1ZUFhClC0t1WbcYQAVvuAheQCmw+kNxBfYrywqscvHOKuhqUZAifgHdRSwXF4QVxQsLLCrYUmgpAikgtDSd3x+nk2SSSTKTTDKZ5PN+vfIqOZnMnKTuztNznvMcmyRJEoiIiIgsKsnsDhARERGFg8EMERERWRqDGSIiIrI0BjNERERkaQxmiIiIyNIYzBAREZGlMZghIiIiS2MwQ0RERJbWxOwORENDQwMOHTqEVq1awWazmd0dIiIi0kCSJJw8eRIdOnRAUpL/8ZeECGYOHTqEvLw8s7tBREREISgvL0dubq7f1xMimGnVqhUA8WWkpaWZ3BsiIiLSoqamBnl5ea77uD8JEczIU0tpaWkMZoiIiCwmWIoIE4CJiIjI0hjMEBERkaUxmCEiIiJLYzBDRERElsZghoiIiCyNwQwRERFZGoMZIiIisjQGM0RERGRpCVE0j4iIKG45ncDmzcDhw0D79kC/fkBystm9iioGM0RERFZVUgJMmAAcPOhuy80F5s8H7Hbz+hVlnGYiIiKyopISYNgwZSADABUVor2kxJx+mYDBDBERkdU4nWJERpJ8X5PbiorEcQmAwQwREZHVbN7sOyLjSZKA8nJxXAJgMENERGQ1hw8be5zFMZghIiKymvbtjT3O4hjMEBERWU2/fmLVks2m/rrNBuTlieMSAIMZIiIiq0lOFsuvAd+ARn4+b17C1JthMENERGRFdjvw3ntAx47K9txc0Z5AdWZYNI+IiMiq7HagsJAVgM3uABEREYUhORno39+US1dXA1deCfz618BrrwEpKaZ0g9NMREREpN/TTwNZWcCRI8DbbwM//WReXzgyQ0RERJpVVQE5Ocq2Rx8FOnQwpz8AgxkiovjCHZQpgqZNA/70J2VbWZlYBW4mBjNERPGCOyhThFRW+tbfe/RRYPZsc/rjjTkzRETxgDsoU4RMneobyFRUxE4gAzCYISKyPu6gTBFQUSHq7/35z+62qVPFf1Jm5seoYTBDRGR13EGZDPbYY2KG0tPhw2IFUyxiMENEZHXcQZkMUl4uRmPmzHG3TZsm4mHvFUyxhAnARERWxx2UyQBFRe7tnmRVVaKWTKzjyAwRkdVxB2UKw4ED4j8Rz0Dm6afFaIwVAhmAwQwRkfVxB2VrcTqBDRuApUvFTxMTs8eNAy64QNlWXS0Sfa2EwQwRUTzgDsrWUFICdO4M5OcDI0eKn507R33p/A8/iDj3xRfdbc88I0Zj2rWLalcMEdFgZtasWbjmmmvQqlUrZGVlYciQIdi7d6/imLNnz2LcuHHIyMhAy5YtMXToUFRVVSmOKSsrw+DBg9GiRQtkZWXh0UcfRX19fSS7TkRkPXa7mDMoLQWKi8XP/fsZyMSKGKkFNHYs0KWLsu3oUbGCyaoiGsxs3LgR48aNw2effYZ169bh3LlzGDRoEE6fPu06ZuLEifi///s/LF++HBs3bsShQ4dg9/gfntPpxODBg1FXV4ctW7bgjTfewJIlS/DUU09FsutERNYk76A8YoT4yaml2BDpWkAapq6++06Mxixa5G6bO1dcPiMjtMvGDCmKjhw5IgGQNm7cKEmSJJ04cUJq2rSptHz5ctcxu3fvlgBIW7dulSRJktauXSslJSVJlZWVrmNeeuklKS0tTaqtrdV0XYfDIQGQHA6HgZ+GiIh0q6+XpNJSSSouFj/r683uUXSUlkqSiBsCP0pL9Z97xQpJys1Vnic3V7Q3GjXK91LHjhn26SJG6/07qjkzDocDANC2bVsAwI4dO3Du3DkMHDjQdUzXrl3RqVMnbN26FQCwdetWdOvWDdnZ2a5jCgoKUFNTg127dqlep7a2FjU1NYoHERGZLEbyRUwRqVpAQaau9i78J2w2YMkS90vz54twpk0bfZeKZVELZhoaGlBUVIS+ffviyiuvBABUVlaiWbNmaN26teLY7OxsVFZWuo7xDGTk1+XX1MyaNQvp6emuR57Z23kSESW6GMkXMU0kagEFmbq6U3oTXR8ZpGg+cQJ45BHtl7CKqAUz48aNw7fffot33nkn4td6/PHH4XA4XI/y8vKIX5OIiPyIxb2j9CyPNmIpdSRqAfnZxmI3usIGCW/jTlfbiy+Krzo9XW/HrSEqwcz48eOxevVqlJaWItdjs4ecnBzU1dXhxIkTiuOrqqqQ01g3OScnx2d1k/w8x09t5ZSUFKSlpSkeRERkkljbO0rPdJdRU2ORqAWkMiV1K97F5ditaHP8bTkefFBHXy0oosGMJEkYP348Vq5ciU8++QQXeFXm6dmzJ5o2bYr169e72vbu3YuysjL07t0bANC7d2988803OHLkiOuYdevWIS0tDZdffnkku09EREaIpb2j9Ex3GT01ZnQtII8pqW9xBWyQsBy3utpexRhIsCGtS6a+81qQTZLUxv2M8dBDD6G4uBirVq3CpZde6mpPT09HamoqAODBBx/E2rVrsWTJEqSlpeHhhx8GAGzZsgWAWJrdo0cPdOjQAbNnz0ZlZSXuuusu3Hffffiz577kAdTU1CA9PR0Oh4OjNERE0bZhgxjRCKa0VCwnjxSnU4yq+BslstlEYLF/v3iu9Vi9y9+dTjEKdfiwCEj69QttCb3TCen8zrBXLMD7uEXx0km0REvbz6H3MUZovn9HckkVANXH4sWLXcecOXNGeuihh6Q2bdpILVq0kG655Rbp8OHDivMcOHBAuummm6TU1FSpXbt20uTJk6Vz585p7geXZhMRmai+XiwVttnUlyPbbJKUlxf5Zdp6lkdHcim1QXbu9O3O6xjl/k5tNsXybCvSev+O6K7ZkoZBn+bNm+OFF17ACy+84PeY888/H2vXrjWya0REFC1yvsiwYWJEw/PeEM29oyIx3RWNqTEvkgQMHgx8+KG7LRn1cCAd5+Fn0ZCbK77TBKn+HNFghoiICIA7X2TCBOXUTTRvulq3gM7K0h5YBVtKbdSUUqMvvgB69lS2vfkmcNdIG7B5jWHXsRoGM0REFB12O1BYaOjNPWLkpdQVFepLyuWcmUBLqUtK1IO3+fN1B2+SBBQUAOvWuduaNweOHQNECmpyZPONYhx3zSYiougxc+8oj1WxQY8Ldym1gSuh/v1vIClJGcgUFwNnzsiBDDGYISKixKC3Cm+oS6kNKhIoSSLeu/Zad1urViKIGTFC0ydJGBFdmh0ruDSbiIhcS7ODTR15L2XWm/diwFL0zz4DGsutuSx75FMMv6U+dqfmIkDr/Zs5M0RElBhCXVWVrDMfJYxVUw0NwHXXAY17LQMAMpKOoaKhPVIW1AELEHLeTTzjNBMRESUOo6vwqglxU8l//UvETZ6BTAnsONqQgRTUuRsTZXNOHTjNREREicfgJdM+59YxndXQAPzylyLRV9a+vYQDSV3QrGK/+jXCqUBsIVrv3xyZISKixBPJVVU6VkJt3CgO9wxkPvgAOFS80X8gA0R/c84Yx2CGiIjIaEGms5yFdvzP/yhTcc4/H6irA26+GbG1OacFMAGYiIgoEvwUCfxkYzIGeN19164FbrrJoyHEvJtExWCGiIgoUjxWQjmdQLduwO7d7pcvukg8b+J9N9ZTgTiS+T8WwWkmIiKiCPvnP0XA4hnI/OMfwL59KoEMoD3vZtUqkWycnw+MHCl+du6ccCudGMwQERFFSH29GH0pKHC3XXaZaB80KMibgy0jBwzbMsHquDSbiIgoAj78EPjNb5RtH38MDBig80Rq00iAGIHxDmRk0Vq6HeEpLlYAJiIiMsG5c2I0pqzM3da9O7BjR4j3ebUKxBs2+A9kAOXS7Ujtpm3gruDh4jQTERGRQf7v/4BmzZSBzIYNwM6dBg+QmL1028BdwY3AkRkiIgoPV9Ogrk7M+njGDj17Atu3A0mRGDYwc+l2sF3BbTaxK3hhYdT+O+DIDBERha6kJOFX07z/PpCSogxkNm8GPv88QoEM4F667b3SSWazAXl57vwaI23erH2KK0oYzBARUWhibKoh2mprgXbtgFtucbf16SMGLq67LsIX17FlguHMnuJSwWCGiCiROJ0iiWPpUvHT6Qz9PIGmGgAx1RDq+WPce+8BzZsDP/3kbtu6Vex8HbHRGG/R2AFcTQxWJ+bSbCKiRGHk6pMNG8SUUjClpeqraSyaZ3P2LJCVBZw86W674QbxMf3N+ERctL9LnbuCh4O7ZhMRkZvRU0LhTDVYNM9m6VIgNVUZyGzfLuI60wIZILI7gPu7nllTXH4wmCEiineRmBLSOoVQVaWc0rJgns2ZM2JKaeRId9uvfw00NADXXGNev0xl1hSXH5xmIiKKd+FOCakJNtUAiL/MPQOkjh3FPI1noomnaFWt1eGtt4C77lK27dgBXHWVOf2JOawATEREURGJ1SfyVMOwYSIIUQtovEd6KioCnzMaVWs1On0aSEsToy+ywYNFUTxTp5RijVp1YhNwmomIKN5FavWJPNXQtq3+PgUSxSW9apYsAVq2VAYyO3cCq1czkIlVHJkhIop3coG1YKtPQi2w5m/aKFRRXNLr6dQpoFUrZVthIbByJYOYWMeRGSKieBep1SdyYrFRIlm1Noi//c03kPn6a1Hdl4FM7GMwQ0SUCCKx+iRYWXs9TFrSe/KkuPSYMe624cPFAFa3blHrBoWJ00xERInCbhfzJkatPgklt8VmEzk2qam+xfvmzYvqkt6XXwYefFDZtmsXcPnlUesCGYTBDBFRIjFy9Yne3BZ59OXVV40NqnRyOIDWrZVtI0cCb7+Nxu0erFeZONExmCEiotAESyz25j36EqklvQFqnyxcCDzyiPLwPXuASy+Fsds9UFQxmCEiotAEqjUjj8JMnw5cfHH0Rjn8BCTHZ76ItvfcrDh01Chg8WKP9w0b5huUyZWJTahqS9qxAjAREYVHLYDIy4t6Doy/gORZTMJk/FXRtm8fcNFFjU/kasb+kpljsDJxotB6/2YwQ0RE4TN7F2yVgOQntEU7KGvgjLmvAa8u8lrIG4ntHsgQ3M6AiIiix+yy9l7LxJ/BY5iCZxSHfI8LceEdrwPor3xvJLZ7oKhiMENERNbXGGgcRQYycVTx0oN4ES9inOI4hUht90BRw6J5RERkfe3bYyae8AlkDuB8dyDTeJwPeVWWv1K/JlYmJm0YzBARxQunU+R/LF0qfnrvWh2njhwBbPn98SRmutomYB4k2HA+ykRDoIAkUts9UNQwmCEiigclJSIBNj9fVIDLzxfPS0qi248oB1QzZgDZ2cq2MnTCPEx0N2gJSCKx3QNFDVczERFZnb8aKfJNPFo343CLzulYEVVZ6Ttj9Ic/AHN6h7lM3OxVWaTApdkeGMwQUdyKlRop4QZUOgKhJ58EZs5Uvv3gQY9BFQYkcYPBjAcGM0QUt2KhRkq4AZXGQOjQId9ZoClTgFmzwv4EFKO03r+ZM0NEZGWxUCPFq8aLD0kCysvFcd6cTjEio/Z3tdxWVIT/92iDTyBz6BADGRIYzBARWVks1EgJJ6AKEgiVSx1hKy/D7Lnu29VTT4k4h2VfSMaieUQUfxIpZyLYztXyFE8ka6SEE1AFCIQm4lnlqiSIxF/v1UtEHJkhovgSK0uUoyUWaqSEU3ROJcD5EZ1gg6QIZP50735IEgMZUsdghojih5xI6j1tUVEh2uM1oDG7Rko4AZVXIPQwFqAzflQccqRDD/xxUSeDO03xhMEMEcUHjYmkcVsV124HDhwQq5aKi8XP/fujU1/G6QTathXff0aG8rVgAVVjILRf6gwbJDyPh10vzcLjkGxJyFz4VPxOE5IhmDNDRPFBz4oaM3d3jiQzdq5Wqw+TmQnccQdQWKgpX+n+f9jxKpTBzlFkICPvPGAeq+9ScAxmiCg+xMIS5UTjrz7M0aNi2ilIIPP998BFFynb5t6/D5Nv+BxovyK+E7fJUJxmIqL4EAtLlBNJmNN6v/+9byBzDG0wec2vgJQUMcLEQIY0YjBDRPEhnBU10RQvO1uHWCjvv/8Vv4rFi91t8zABEmxogxPxn6xNEcFghojiQywsUQ4mnpaN653Wczpx16BKXHqp8uXjaI0JWOBuSIRkbTIcgxkiih9mL1EOJN6WjeuY1tu9YB1sTZLx1rocV/PzGAcJNrSGw/c9gbY/IFLBjSaJKP7EWgXgWNnZ2kjyZwpSefi23H9h2dY8xUsOpCENJ4Nfo7gYGDHCmP6SJcXERpObNm3CzTffjA4dOsBms+H9999XvC5JEp566im0b98eqampGDhwIPbt26c45tixY7jjjjuQlpaG1q1bY/To0Th16lQku01EVicvUR4xIjYSScPZiDFWBZnW+1a6ArbyMkUg8wrGQoJNWyADMFmbNItoMHP69Gl0794dL7zwgurrs2fPxoIFC/Dyyy9j27ZtOO+881BQUICzZ8+6jrnjjjuwa9curFu3DqtXr8amTZswduzYSHabiMg4Tiewfr22Y622bFxlWk8CYG++Bt3wjeLQk2iJsVik7byxkqxN1iFFCQBp5cqVrucNDQ1STk6ONGfOHFfbiRMnpJSUFGnp0qWSJEnSf/7zHwmA9O9//9t1zIcffijZbDapoqJC87UdDocEQHI4HOF/ECIirVaskKTcXEkSYy/BH6Wlxl6/vl6cs7hY/KyvN/b8XtfZOWutz0d6HaO0f35Akmw28VixIjJ9JUvRev82LQF4//79qKysxMCBA11t6enp6NWrF7Zu3QoA2Lp1K1q3bo2rr77adczAgQORlJSEbdu2+T13bW0tampqFA8iIt3CWUbtL+FXTSRGItRWTrVvD0ycaPiScCkpGb+d2x89Hr/J1WazAafWbsK9WKLvZLGQrE2WY1owU1lZCQDI9toCNTs72/VaZWUlsrKyFK83adIEbdu2dR2jZtasWUhPT3c98vLy/B5LRKQqnGXUgQrKeYvEsnF/gVR1tbiOgUvCv/wSSEoC1qxxt73xBtDQAJw3qG/w2j+5ucDHH0d/PymKK3G5NPvxxx+Hw+FwPcrLy83uEhFZSbjLqIMl/HoyeiRCayB18CAwdCiwfHlIl5EkYNAg4Kqr3G0pKcDp08Dddzc2aKn9M38+MGBA7CRrkyWZFszk5Ih6A1VVVYr2qqoq12s5OTk4cuSI4vX6+nocO3bMdYyalJQUpKWlKR5ERJoYsfu21kTeJ580fiRCTyAFiCDivfd0XeLzz8VozLp17ra33wbOngVatPA62F/tn3btgGXLOApDhjAtmLnggguQk5OD9R5Z/jU1Ndi2bRt69+4NAOjduzdOnDiBHTt2uI755JNP0NDQgF69ekW9z0SUAIxYRq11SfGAAcaPROhdEeV0AsOHa5pykiTgV78CrrnG3dayJfDzz2Imzi+7HXj2WRHAyKqrRf6O1YoFUkyKaDBz6tQp7Ny5Ezt37gQgkn537tyJsrIy2Gw2FBUV4emnn8YHH3yAb775BnfffTc6dOiAIUOGAAAuu+wy3HjjjRgzZgy2b9+Of/3rXxg/fjxuv/12dOjQIZJdJ6JEZcTu22buExVqbZYgo03btonRmNJSd9u77wInTwKpqUHOXVIC3Hab2E3bk1WrH1PsieSSqtLSUgmi7IDicc8990iSJJZn//GPf5Sys7OllJQUacCAAdLevXsV5/jpp5+kESNGSC1btpTS0tKke++9Vzp58qSufnBpNhFpVlpqzDLqFSvcy4yjufS4vl4sB/e+boifqaFBkvr2VR7Wpo0knT2rsz+BlmLn5YW2bDxaS8/JNFrv39zOgIjIk8Yy/Zq2HigpEfk3ntNWeXliRVEkc0VKSkRyr15e2wds2QL07as8ZMUKla4H2j5iwwaxeiqY0lKRAKyV2nebmysSipmHEzdiYjsDIiLLMXL3bbsdOHBA3KijufTYbgemT9f/vsYpqoYG4NprlYFMdjZQW6vS9WBL2I2YtvMWb5t2UtgYzBBR/Amn2B1g7O7bZu0TVV+v7/iMDKBfP2zaJLr473+7X1q1CqisBJo183qPlqBCx+7amhix2oziThOzO0BEZCijph/sdqCwMDq7b8fALt9OKQk9eybhq6/cbXl5wPffA02bqr0hSFBhs4mg4rvvxPcfbNpOazK0ntVmeqatyNI4MkNE8cPo6YdojKqEU2k4EB038lL0R5NjR/DVV+5ptTVrgLIyP4EMoD2o2LLFuGk7IDLTVmR5DGaIKD5onX6oqwtvCspIkcz96N9fTB0F4EQSLscu/Aru9dZdugDnzgG/+U2Q8+sJKoyctjN62oriAlczEVF80LpqJjNTFGyTeU9BRWvKR1415W90Q8+qKX8CrGpah4EYhHWKto+e+QoFj3XXdu5QVikZ8d0audqMYp7W+zeDGSKKD0uXBilD64c81SGX9I/Wct9ILVn25HQCM2cCzz0HnDgBAKhHMi7Hf7APl7gO64rd+Cb3N2hy4DvtAYCZQYU8ogUor+35u+Ty7LjApdlElFhCnVaQb4Zjxxo/5RNoVdWqVdrOEWruh5yLM22aK5D5CAVoinpFIPMxBmC37Qo0mf9XfUGHkUvY9TJy2oriAkdmiCg+BBspCEcoowyBVlUVForgy3O6y59QRmbkkYvG7+EcmuBi7MOP6Ow6pBu+xpf4BZLRADz6KDB7tr5reF7LjMKAQEysAqPI4jSTBwYzRAnC3/SDUbQGFl7BhIs8YjF9uhgxCSYzUwQJW7Zov2F75eKsxmDcjNXKj4H+6I+N7j4FC9SCBQ0MKihCtN6/WWeGiOKHPP3gPVLgnfQbKi1TPlrqryxYoO16vXqJ5UX+RnfUAojGJdN1aIoLsB+H4J6K6YnPsR3XIgkefQtWl0VL3R55CTuRSRjMEFF8USt216ePCArCnYLSkpejpf7KTz9pu97q1b5tFRVihVJGhvI8coBRW4v3UYhb8L7ibZvQD/3wqf9rqQVq/kaY5Dwi5qdQjGAwQ0Txx3ukwOkExowJPLWTlgacPBl+lVqtCbtt2gDHj6u/ZrMBSUnqNXDk/nkHRBUVqB06Eh1bHMNPcG8W2QufYQv6KEdj1HgHalor/BYWckqJTMfVTEQU3zxX9QRSU+M/kAG0r8zRs8eQGptN9ENnMb/3JDua4yx++rmFq20LeuMz9A4cyNhsImHXO1DTs20AkckYzBBR/PJXYVcPvct9+/UT7/Feruytpka9vW1bMeKh0VmkIB0nMBzvudquP/9HNCAJvW3bAr85UKDGbQPIQhjMEFF8CjRNEkxmJvDWW2L10v79+nfJDlZ/JZDUVOC3v9V0qXdwG1JxFjVId7Vtw7XYOGsLbCtU6rB4ByyBArVE2jYg3F3WyXTMmSGi+BRsmiSQ6moRCIS6Qsffqqp27YKvqpKPD7DT9Bk0RwZ+whm4p5QG4GOsw69hA0SA0b+/eiK01mXe8ghTsAq/ffqIAMCqy7KN2mWdTMVghojiU7jTH+G+X21VVUUFcOedwd975Ii4mQ4b5s6hafQW7sBdeEtx+OfoiZ74wjdRWW3JtNYATR5hUumDa4Tp9tv9Lx23QiDA1Vpxg9NMRBSfwp3+MGL6RA4mRowQP72nfQJd26tk/89IRROcUwQyN2EtGmBzBzIAcN99wLJlxkyXBNo24A9/AObOjcyO39GgdZd1TjlZAisAE1F8CrYrtT+R3CAxlM0ZnU68MfW/GPXMZYpDv8wqQI8j/3Q3ZGSIn2q1Z8IdXfCu8CvX7Ynkjt+RFo2NPils3GiSiBKbZyKuVpHeIDFQcjAgApzRo10jK6ccTtiaJCsCmcLfSWhoAHocWitutMXFwIwZIohRqT1jyCiJ9wjTli3WX7bN1VpxhcEMEcUvu13c6LVq104EEpHMk/A3dSObPh0YORJ/y38LrVorA6qv0Q3vf9EJtpUl7gDj1luBRYvUzxWp6ZJ4CAQSabVWAmAwQ0Tx7bLLgh8jq64GJk6MfL6H3Q48+6zqSyfREjZIGIO/udqG4j00wIZu+NZ3tMWM4nbxEAgEqwfkr5ggxSQGM0QUn5xOYP164IEH9L0vGgmsTicwaZJP88u4H2k4qWjbhcvxHobDdcv1Hm0xY5QkHgIBLfWAIjXdSIZjMENE8UfewmDgQODYMX3vjeRKFrk42/TpitEUB9Jgg4QH8bKr7XYshQQbLsdu9T7Koy1mjJLESyAQaLUWl2VbCuvMEFFs815JI/+1790m3zj91Q7RwzNYMGoli1pxNgALMR6PYKGibTe6oiv2Bj/n+vVA164i1+foUfVj9GySqYe/woC5uSKQsUogoFYPyGqF/4hLs4kohqkFAIGWIBcWhrYc25+33hJ/tYd7k1MJsE4gHW1wQnHY3XgDb2BUeH32JI+SRHKUQS3YZCBABtF6/2YwQ0SxSc8Ii3zTnj49+O7YemRmKrcf8Fe3JdANXaXezXMowiQ8pzjFf3ExLsZ3xvUdEHkrgUZJGIhQjGMw44HBDJHFhFLwzmYTO05711oxktpIR7C9fTyKsx1DG2RAmcMzGn/D3zDGuD4mJQHjxwO33BI4OOGeRGQBLJpHROYKZyfiUDaJlCR9gYw8XaVlJ2vPawDu5GB59ChQSf/GVURz8AefQOY7dAkcyGRkuPupVUMDsGCBSHwOFMgE6zeRhXBkhoiMF+5f/UuXAiNHhnbttm2B48f9T09lZADvvisSe1et8u2n99SSPx9/DIwaFbSk/9F5byFz6PWKlx7Ei3gR45THP/mkO9n4yBEgK0v8u7JS9CczE9izB3j66eB9A8QUk9p2AsFGvaywFQElDK33b65mIiJjGbETcTjLiCdMELkz/nZ6fvVVYMAA8e9wdrbesCFosbrN5efjeq9AZj86ozN+VPYrN1f0GRB92b4dePtt33ydMTqmo/ytxtJTZI97EpFFcJqJiIxj1E7EwYqyqZELtU2dqq92SKg7WwdwGi0wAfNwAza62h7GAkiw+QYygEjSXbVKjJjk54vn3qNDFRUiuVnPtJNaoTwji+yFM5VIZCAGM0RkHKNK6wfbkNGbd6E2ux04cMC9EWNpqZg2KSwMfvPVWt3WzxTMRlyP/8HXWIAJkJCE+7AIlcjGAkzwPVgOsAD1HBZPkqQvuAPUR7iMKrInFybMzxdTgvn54jnzbcgMUgJwOBwSAMnhcJjdFaL4VlwsSeK2G/hRXKztfCtWSFJurvK9GRni4dmWlyeO1Xuu3Fz1961YIUk2m3h4Hi+3LVvmc66TOE8ah4XuLuFH6SMM8v8dZGZKUm2tJNXX+/Yr2GPaNElKTvb/us0mvpP6et/PJl/P+7Npea/396P2Xpst+O+CSCOt92+OzBCRcYwura82wlJVJR7eoy6B8nD0rt4JVuY+M1Nxrk+Qj274Bi9gPABgLF7Bt7gSBfin/z5VVwNbtoS2cuvSS8Xokppg2wmEshWB53TS+vXAI4+EP5VIZKQoBVem4sgMUZQY8Ve/v/OWlooRndJSfe/XMvLRurUkvfmm77n9XbdxBKoGLaUH8KLrNOdjv7QOA7SPsBQXax/N8nyUlop+qI02aRml0vNeteP09JEoDFrv31zNRETGkf/qHzbM/2oivRsQhrvMW8vIx4kTwN13+55bTg721r49PsYAjMZrKMP5AMRy62fw/9AKpzR9LABi+bWe78J7n6XCQiA9XYyaAKKv/ftrO6eWPYnC2efKyF26iYKJUnBlKo7MEEVZOCMG3ucJNzcjlJGPAOd2OCRpzH1O16Gd8YO0Hvm+5wiU0yI/cnPd+Tf+RrO8H0VFYtRDJW/Hbw5QKELJ5eHIDBlM6/2bRfOIKDLC3ffHqOJuHtsJaObn3P/4hyj1Ul4uno/HQszC42iJ077nKCwEPvhA/Nvf/83Ko1V/+AMwd27gY5OTg+ehGLmxZCjfm9wHFt0jg3A7AyIyl3f9Fr03NqOWeYdSs8br3A4HcN99wI03iuYLL5SwIXM4FuIR9UAGEMm9S5cCHToEvg4AvPMOsGyZb8JxZibw29+Kf2tJqDUyATeUaaJQpxKJwsRghohii7xyZsUKbccHu+l6rt7R6/BhrF0LXHEF8Npr4l49YQLw9fObcUP1e4HfW10tNny8777Ax8mBU7t2viu3Dh4Edu7U12etQV4woVRh9leYkCjCmABMRLFDLdk3GC03XXmp9QMPaNt3CcBxtMakN/tjyUfi+UUXAa+/3ph7u7RCW9+OHgVmzNB27OHDvgnHwbZMCHa+cMgjWhUV6lNfNpsYSVqyROwlFcpUIpFBGMwQkTm8c2qqq4HbbtO+csZ7ZU8wdruYsunYUQQZAazGb3F/8iIc+igHNpuYtXn6aaBFi8YDwtk7yh+1c4YTkITbRy0r0+bPd+9zRWQiBjNEFB2ewcu+fWLDxwqPEY7kZH2BDKA/N6NZM+CVV/wuNz6GNijCPPwddwNO4JJLgMWLgT69Gqe+5CXQwUYt9AgUlIUSkMjn69NH9DfUBGzAPaKltjR+3jxOJ1HMYDBDRJGnZfpIT8JqODdTPzfoD3Az7scrqER7JCVJmDTJhj/9CUj9sATIHgv89JPyPC1b6g9kvEc4APF86FAR6HkHHHqDJjnIu/12oEuX0GvzeNJSj4bIZFyaTUTahbLcOpzCa97Gjxc3fiNupo2fZd+OGlzyh9+5mrt2lbB4sQ2//CVE34cODXyetDSgpib49WbMABYtUgYY3sut1QIO+fsDgn+HeXkikJk71/dYI5dtE0WJ1vs3gxki0kZrJV7PgCcrCxg1KvQkVm+lpeoVeUN0993A3//ufv7YYyLmaN4cwevcyDp0AOrq/OfheNZdAcR3s2qVGFlSOxbwDTjUvvu8PODZZ8UqKDm47NPHd0TGX184skIWwGDGA4MZSgjhFqkLxN/oivfNN5TVSFqoBQRhFOPb/dYOXD7qWkXz888D48Z5NOgpGjdjBjB9uvi3WqKsZ3ASajFALb9frX02OCgkihSt92/mzBBZjdpNbdWq8PYvCna9CRPUpzgkCa7lPk6nvtVIWnkm+4b7OUtKcPudTfDumd8pmh1//wBpdyrbdK0kuvhi9UTZjh1FyeDaWhFo9OunrxigZ8Dhb5+oUPrMfZMozjCYIbIStZGPjAzf5FRAJI0OGxZ+joTWm+9DDxkTyKjlkchTMmqjQxo/565563DlROXrL+N+3G9bBNwNoIXX+/WsJGrfXgQanomy+/aJHJlp05SfRc5/CSaUgENrnyOxtJzIRJxmIrKKUBJpjciRWLoUGDkytPfqIY/ALFumzAORlywHm5rxU8BNkoChdgkr31duZ1CDVu4drtW+J605M7m5onKvUbtNy0KZCpL7HKjQHXNmyEK4NxNRPAk01ROIEaXto/VXvFwKf9gw3z2dtIwOHTwIDBwoAq/8fKBzZ3z97MdISoIikPkbRkOCzR3IyO/3/p60boMwf75vbksovyuZzSaSe7UWA/Tk2We1vagkKfj2CkQWxGCGyAqC3cyDCSdHQstGjZmZ+s+bmyumYJ58EnjiCREAnDkjcku8a87o7L8E4HcHX0D3yQMV7adwHkbjdf9v9L6O3S72iMrI8D02I0O85j21Fc7vyoiNGuU6Ot6bVsqmTROjNyUl6q/Le2MtXar+uyCKQcyZIbKCcBM2wxldCVTWXnbmTPB6K+3aAWPHAklJYsTl+HFg4kT1G793Uq+O/n+JHrgKXyraljy2C/fMvjL4m9WuIxeN86wA3L+//53A9fyu/OUHhZu0Lfd55kxlzo7MX56R1uX3RDGGOTNEVqBnmbAnI3MkSkpEMKKWbKyXv6RlTzab+2YbLBcEYjTmJnyIf+BGV1tT1OEEWqPFExNFMu7Ro5HPJdH7u3ruOSA72/jl9HqXgGtdfk8URcyZIYonWqZ6vBkxZeGpsLCxmpwBtAZE8pJveXTITyCzA1chCZIikHkLd6AOKWiBM8Cf/yw2svQXyAC+31Oo0y3y70qr7GxlfpBR9CwBD7b8HnD/LohikGWCmRdeeAGdO3dG8+bN0atXL2zfvt3sLhFFT6DETvm5d16HnFBr1F/TmzcrN4YMJDMTePPN0HJpZN5JuXa7KE7neQiAX2E9rsYOV9t5OIWfkYo7UKztOmrfU0mJGNXIz1ckFPvNM/GkNXFYFqkEaz01Z/QEPkQxyBLBzLvvvotJkyZh2rRp+OKLL9C9e3cUFBTgyJEjZneNKHr8JXbm5opE1KoqsZy3uFj83L/f2GkBPbkg1dVi9KW62tjrXnyx65/bcQ2SIKEUv3K1vYtbcQqtkIqzgc+ZmQm89Zb69yRPt3jf3OU8Ey0Bjd0OLF8efKQl1FVLWuipOcNie2RxlkgAfvbZZzFmzBjce++9AICXX34Za9asweuvv44pU6aY3DuiKAq2g7HWuiShbH2gdwTh++/1Ha/luu3bQwJwAzZiM653NbfGcRxGezRHrbZzVleLoND7+9JS7XjCBLFjtjxK4S8ZeNgwMUV1663++3H77ZGr9xJsx205Z0auSqwFi+1RrJJiXG1trZScnCytXLlS0X733XdLv/vd71Tfc/bsWcnhcLge5eXlEgDJ4XBEocdEMW7FCknKzZUkcYsTj9xc0R5Ifb3v+wI9nntO+7FqD5tNkvLyxHUb/WtTvc9h78Ee2vmLi30/Y2lpaOfKyPD//T36aODPGOx7D8eKFeIaNpvvdT2vLf9uvY8L8LsgigaHw6Hp/h3z00xHjx6F0+lEdna2oj07OxuVlZWq75k1axbS09Ndj7y8vGh0lSj2hTOFojUXRC769tBD+pOWvTUm5TY0ANdeC/S93j2KkYUq1KIZhkLDtI8atVGGUKdRfvoJGDrU9/tzOsXoTCCRTKwNNDXpmSekJSfLqERyogiI+WAmFI8//jgcDofrUV5ebnaXiMxnxIqVQEXkAOWNr1mzgCuQAsrLc91sN28W99B//9v98vsoRBVy0Azn9J87UIXdcKdRJkxQfn+xkFhrt4vtFoLlU2kNfIhiUMwHM+3atUNycjKqqqoU7VVVVcjJyVF9T0pKCtLS0hQPooRn1I3VbhfJxtOmidwRTx07Km98KiuQ/Bo3TpGU2zDEjquuAq53p8agY0egds4CFOIDbef0FmyUIZQl8J4OHgQWLnQHNLGSWCvvuB1sCbjWwIcoxsR8MNOsWTP07NkT69evd7U1NDRg/fr16N27t4k9I7IYI2+sq1YBzz8PnDqlbD9zxvdYjxVIAfXtC9xxB9C/P0o3JSM5GfjSo5Dv6tUiVmjW3s+okBbBRhmC1LPRZOJE9zJuK+5irTXwIYohMR/MAMCkSZOwaNEivPHGG9i9ezcefPBBnD592rW6iYg0MOrGWlIi8kPUCt+p5Y7ouK7TCVx5JfAr92prdO4M1NUBgwc3Nvjbc0iNzRZ4GbaawkL/02hayTlI1dWBR3rC2VSSiFwsEczcdtttmDt3Lp566in06NEDO3fuxEcffeSTFExEAQSbQtFyY3U6gUceCX4tz9wRjdf9uLYfmjQBdu1yv/ThhyL+aNpU5XNoIUnKZdhaRhk2bw5/ywZ5ZGfyZLFdAcDEWqIIskQwAwDjx4/Hjz/+iNraWmzbtg29evUyu0tE1mLEihWtVYAPHnTn3gSZuqmXknBp/S78+kb3dS+9VMK5c8CNN6q8QT6fnrwWPTkpRuWvyDlI7doxsZYowiwTzBBREFr2Egp3xYrBQcE/MAhNUY//Hm7laluHgdhzuhOafFDi/zPJn0PrdgnBpro8r+O12CBshw8zsZYowrhrNlE8KCkRUzueq5Vyc8UIhtoNM5QKwIC+HaFLS8XUjsruzfVIxiX4L/bjQlfbFfgWX6E7ktEgRl0kyXd3be/PVFcnArOjR9X7oGU3bLXvLjk58BL1du2AggLg7bcDfweA+3sgIt24azZRotBTCE8egVi2TDy/9VZ9K1b69dOWgCuXyQd8loSvwW/QFPWKQOYT5ONbdBOBDOCekvLOXfH+TM2aAa+8IoKWUKbO/H13/gIZ+TqvvAK88QaTe4liBIMZokjQMuVj1HW0FsILZydoWXIysGBB8OPmz3cHEKtWAQDOoQlyUY7fYo3rsKuwA04kIR8btF1frbhfqFNngb47mXcQ5HlOVs0lihmcZiIymt4pn3BonfaZMQOYPt33xi3fdPUmopaUAGPH+o6cZGQAL70kclkOHwaysoDbb8eqo30wBKsUh25CP/TDp9qv6c17+kbv1JnW727uXKBDB//nVPt95+WJQIY5MURh0Xr/ZjBDZCR52sKooCGYpUvFKEswbdsCx46pv6Ylr0SNPPq0YYN43r8/cPy4KBrXeGOvQ1Pk4iCqkeV6Wy98hi3ogySE+X89xcWisFuo9Hx3ixYFzj2qqBBLwDMzxQiR1hwkIgqIwYwHBjMUFSqJrgqhBg2B6EnIDSbcRFWvQG4F7BiGFYpD/oU+6IOtYXTSw8cfi+9RbxKzTO93N2MGMHWq+xrRHIEjSlAMZjwwmKGo0HpzNHJ1ixxAVVSo537YbECbNv5HZTyFM9LhEcidRQpyUAkHWrtevg6bsQnXI4z9s91sNjFakpoaXiDhdALZ2foK5MnXAKI7AkeUoLiaiSjazNhUUEsS6oQJ2s4Vzv5AjSuW3sWtSMVZRSDzGXphc6BAJiNDLHXWSpJEAKJl9VYgq1bpr/R78KC4xtix4e0+TkSGYjBDZBSzNhUMtppn6tSILyE+8+MRnIdTuB3vutp+hfVogA29sN3/G2024NVXRSCiJaDp2NH/vkl6Agl5JVMo5GAq0Otadh8nIsMwmCEyihF7H4UqUIXZCC8hfvttoMWoW/EzznO1/RtXYz0GBp5WSk4W9W7s9sD1YmQzZojaLkYEEl61byLCyBE4IgqIwQyRUYwOGvTWqklOFrk4I0b4FsILdxsDFT//LGKQO+90txXgIzTAhquxI/gJnE4xGiN/ztpasXy8QwflcXl5wIoVwFNPAUeOaOtcsEAiGoGG0SNwRORXE7M7QBRX5KBBbZWLnrojkVgpY7cDhYWhbWPg5Y03gFGjlG1fzFmPXzx6k74TrVoF3HWX7+ecMQO4+GLfPho1lRfJQENetcbKv0RRw9VMRJEQ6t5HQPRr1ehw+jTQsqWy7eabRUxiszX27fbbw0t+DfQ56+qAFi0Cnz852T1s5E+wVWDB+te2rZjukveQ0tJ3ItKNq5mIzBRoyicQPdsTRNnrr/sGMl99BXzwQeM9XJ42GjdO2wn9fSeBPueWLdqSe7dsCX7tYFOCauTXXn1VTH0ZOG1HRKHjNBPFl3BGRGJBsMRUOcF14UJRI0X+jPJ7I/C5T54EvP8gGjoUWL7c476vNi2WlAQ0NPieUB7NCBSUeCbyetbkMXL5e7ApQSD4dKFB03ZEFB4GMxQ/4qEiq9ab9cSJ7n/LS5U9V/kY9LlfeQV44AFl27ffAldc4dHgb1pMft6yJXDqlLJvQ4e6A4ZAvL8Po5e/B8sjChasyCNwRGQq5sxQfIjhPBNdjNqeIMzPXVMDpKcr22775Y94Z9Z+5Q1dyxYOHTsCS5aIlUhyQLBhAzBwYPCOfPwxMGCA+7mWisdGbxlBRKZhzgwljhjOM9EtWK0arcL43M8/7xvI7EZXvPNZZxFode7srrKrZVrs4EERWOjNH1IT4Zo5RGRNDGbI+rTmmVihImtyMvDss/pX2KjR+blPnBDxwMMPu9vuwpuQYENX7HU3em4bEGoOi9Z6MWrHRaBmDhFZG4MZsj4z9kSKlJISYNIkY8+p4XPPmyf2o/S0N/t6vIl7fA/2HPXJytLWB+8clnBzXwJVPCaihMMEYLI+s/ZECkWg1Vb+8n7CFeBzHzvmu9XR738PvHbXBiA/wIiOPOoDiBGRYDks3gXk5Ok0ve/zxORbImrEkRmyPjP3RNKjpETkm+TnAyNHKvNPAuX9hCrI55471zeQ+e474LXXoH0U68iR0HJYmPtCRAZiMEPWZ4Ubozzq4p3bI+efzJxp7MaHAT63XLj20UfdbfffL+KoLl0aG/SMdoWaw8LcFyIyCJdmU/xQqzOTl6dvT6RI0LJ8uU0bMedjFD+fe9Ys4IknlIf+8ANwwQVe7w9lCXSoBQutXuiQiCJG6/2bwQzFl1i8MRpVO0aLoiJR6M3rc1dX++bqPvwwsGBBgHPJo0kA9x8iIlNovX8zAZjiSywmhWrNP2nbFjh+PHDeTG6uqNeydKnmEaj//V/gqaeUbT/+CHTqFKQ//sr9t2sH3HGH6K/TaX6wSEQJjzkzRJGmNf9kwgTx018i84wZYjny7NmaliVXVYlTeQYyEyeKWCloICPzXAJdVARkZophnnnzfAvoERGZhNNMRJGmJ/9k1SpD8n7++Efg6aeVbeXl4jIhiZftIojIUpgz44HBDJlOT/5JGHk/hw8DHToo2x57DHjmmRD6LPejokKMyhw9qn4c90MioghhMOOBwQzFhAivtpoyxTdoqajwDW40UetrMKWlsZevRESWxgRgolhjt4uVRgavtjp4UMREnqZO9Z1m0izUSsRW2C6CiOISgxmiaDJ4tdUf/gD89a/KtsOHgZycEE8YTiXiWNgugogSElczEVlQWZlIVfEMZKZPFzFIWIHMwoWhVSIOto8SEVEEcWSGyGImPNKABQuVf4dUVWnfwFpVKDkynsaMYfIvEZmGIzNEFnHggBiN8QxkZuIJSLl5yPo0jFov/vaN0uPii0N/LxFRmBjMEFnAQw/57p9UjXZ4ArPcm1WGUrzOqN26mS9DRCbiNBPFtljcaylcOj7TDz947GTd6Bk8hscwx90gByITJojVUnq+n82bwxuRkWvM+MuXicffHxHFHI7MUOwqKRGVc/PzgZEj46N8vo7PNGaMbyDzE9oqAxlPBw8CM2fq60+4y6klSdTJUQtQ4vH3R0QxicEMxSZ/eRzhTKmYTeNn2rdPDHj87W/uQ5698wtIsKEtjge+xrRpopqwVuFODxUVqRf8i8ffHxHFLFYAptgj72Xkb/rDiuXzNX6me/ofwJt/V/6NcewY0OarDWJkQ4vkZLGr9vDh2vvlb9+oYNSq/sbj74+ITKH1/s2RGYo9wfI4JEnsmrh5c/T6FK4gn2mPdAls5WWKQGbBAvFR27SByDXRukuk0wncequ20Y/kZGD+fPFvf7t1q7HZRNlhtVyZePz9EVFMYzBDsUdrHoeVyucH6OsIFOMy7FG0nTgBPPywR4Nn0KFVUZEIbIKx28XUVMeOyvaMDPHTO8iRn/vLlYnH3x8RxTQGMxR7tOZxWGk5sEpfd+Fy2CDhHYxwtb008b+QJCA9XeUcdjswY4b2a+oZ/bDbRSGb0lKguFj8rKoCVqzwDXJyc5W7fHuLx98fEcU05sxQ7AmWx2HFnAuPzyRJEoZjOVZgmOKQmo6XodWP3wb+TE4ncP754rvR4sknxT4H4XxPepdXx+Pvj4hMwZwZsq5AeRzBpjhiVeNn+ka6EkmQFIHMIoyBZEtCqwUzg3+m5GSRTKPV00+Hvxxa3hxzxAjxU0sf4+33R0QxjcEMxSZ/eRzBpjiM5nQCGzaI1UEbNmjLQVEhScDvltjxP/ha0X4SLXFf3j/0fSa7HVi+XHswYMZy6Fj5/RFRQuA0E8U2MyvIqm2+mJsrRh103Ix37gR+8Qtl2+L/twejun8Z3mdavlysWtLCrKkdVgAmojBovX8zmCFSIxd98/6fhzxNomF0QZKAwYOBDz90tyUnAw4HcN55BvZTz27XanVhiIhiFHNmiEIVaPNFuS3IsucdO4CkJGUg8/e/A/X1BgYygHsV0pNPajuey6GJKA4xmCHyFkbRN0kCBg4Err7a3ZaaCvz8M3DnnRHoKyCGewYM0HYsl0MTURxiMEPkLcSib9u3i9GY9evdbUuXikAmNVXl/QYlFwNwVwj2V8U3UMVeIiKLYzBD5G3fPm3HNY5ySBJw/fVAr17ul9LTgTNngNtv9/Neo3eU5nJoIkpgDGaIPDmdwKJFwY/LzQX69cOWLWI0xnPGaflysR1B8+Z+3hupHaULC0WBvDZtfPvK5dBEFMeamN0BopgSLF+mUcPoMejTNxnbtrnb2rU6i4PvbUNK/97Ahi3qy5GDJRfbbCK5uLBQ3yiK2qqmtm1F29SpHJEhorjGYIbIk4Z8mU/RF/1mPKVoW4khGHJyFVAAETh45r941qbRk1ysdQm1v2Xkx4+LkZorr+SoDBHFNU4zEXkKsNqnATZchR3oh0/dh+MQatEMQ7DKfaB3Iq/n9JHRO0obsIyciMjqIhbMzJw5E3369EGLFi3QunVr1WPKysowePBgtGjRAllZWXj00UdRX1+vOGbDhg246qqrkJKSgosuughLliyJVJeJ/K4K2oAbkIwGfImrXG0fZNyLQ+iIZjgX+JyeQUVWlrZ+aF1CHcYyciKieBGxaaa6ujoMHz4cvXv3xmuvvebzutPpxODBg5GTk4MtW7bg8OHDuPvuu9G0aVP8+c9/BgDs378fgwcPxgMPPIC3334b69evx3333Yf27dujoKAgUl2nRCavCho2DLDZ4JRs+AW+xDf4H9ch52eexr7iz9H010u0n1cOKgARLAXbUVrrEmqjR3q08tymQA7QjhzhlgVEZA4pwhYvXiylp6f7tK9du1ZKSkqSKisrXW0vvfSSlJaWJtXW1kqSJEmPPfaYdMUVVyjed9ttt0kFBQW6+uBwOCQAksPh0P8BKDGtWCF93O42SUQc7sfaqZ+K14uLJZ8XtTyKiyVpxQpJstnEw/M1uW3FCu39LC3Vdt0ZMwz9bqTcXP/Xys3V9xmIiPzQev82LWdm69at6NatG7Kzs11tBQUFqKmpwa5du1zHDBw4UPG+goICbN26NeC5a2trUVNTo3gQaVVfD3R9wo6BR99xtV3U8WecO+vETU/3FQ2hVtJt397YHaX79fM9j5pFi4zJm/G3rNzTwYPA0KHisxARRYFpwUxlZaUikAHgel5ZWRnwmJqaGpw5c8bvuWfNmoX09HTXIy8vz+DeU7z65z+Bpk2BvXuVbfsOtkCTFI+pk2AVd715V+CV91QqLQWKi8XP/fv1rzpKTgbGjg1+3MGD4efNBEo2VnPbbaLoDhFRhOkKZqZMmQKbzRbwsWfPnkj1VbPHH38cDofD9SiXcxWI/KivB7p0ATxTsa7ofBr1tU78+tcqbwhUcdebvwq8ycli+fWIEeJnqHkmXbpoOy7cvBmNNXhcGhqAW28NvQggEZFGuoKZyZMnY/fu3QEfF154oaZz5eTkoKqqStEmP8/JyQl4TFpaGlJVN7sRUlJSkJaWpngQ+bN2rRiN+eEHd9t6/ArfHmiJ5C6d/d+M/U0XeQclkazAW1IiVklpEe4mk6EGQ1waTkQRpms1U2ZmJjIzMw25cO/evTFz5kwcOXIEWY2rIdatW4e0tDRcfvnlrmPWrl2reN+6devQu3dvQ/pAie3cOeDCC5WDDd2xEzvQE8loEA1yjRh/wYjdLqr1yit72rcH+vQBtvipAGwkf8XyvOldIeVPqMGQ3iKAREQ6RWxpdllZGY4dO4aysjI4nU7s3LkTAHDRRRehZcuWGDRoEC6//HLcddddmD17NiorK/Hkk09i3LhxSElJAQA88MADeP755/HYY4/h97//PT755BMsW7YMa9asiVS3KUF88IGIQTxtxPW4Hl55JVq2GJCnizxF+satNX/FyE0m5Twhf8vKAzF6aTgRkadILae65557JAA+j9LSUtcxBw4ckG666SYpNTVVateunTR58mTp3LlzivOUlpZKPXr0kJo1ayZdeOGF0uLFi3X3hUuzSVZbK0lZWcqVxNd0dUhO2IIvb/b4b9d0WpdkZ2Yau0za37JyK313RGQZWu/fNknS+yeW9dTU1CA9PR0Oh4P5M/FALthWUQFUVwOZmSJvJch0TkmJWDHs6dNPgb5lS4GRI4Nft7hYJOvGgqUa+/zWW8Addxh7bbVNLf2Rp7j272chPSLSTev9mxtNkrUEupF6bujoobZWpHscP+5u69sX2LQJSEoCcE5jLki4CbRG0toXLTVo9PLME1q1CnjtNeDkSd/jjJziIiIKgCMzicyzJL0VytBrSXi12RTJusuWiXInnrZuBX75S48GpxPo3Dn4FgOxNLoQS312OoGZM0UgeeyYuz0vTwQy3LGbiEKk9f7NYCZRqY1w+BnZiAnyzVvL1EZeHs7u3o922ck4fdrd3L8/8MknfsrCyIESoAwObDbxXE4AjqWAL1CfgcgtB/fHasExEcU8rfdv0yoAk4n8laSvqBBJJRMnAhs2xFZtEB0F25aW90VqS2Ugs327KLLrt76dv5oxSY3/E5k3D8jPFwFVrBSBM3JbBCMYVQSQiEgnjswkGj0jHLE0UqMh4fVnpKINjqMOKa62QYOAjz7SvuuAa3Rh1SoRwHgza9QjEI6IEFGc4jSTBwYzHjZsECMMWsTSjTtIv9/EXbgHbyraduwArroqhGsFC/hiMYeGiCgOcZqJ1OkpXibHubFQjt7Pxo6n0QJJcCoCmcHN16PhnDO0QAYIPqUlSe6qtpHidIoAbunS2JvyIyKKMQxmEo3e5cXRuHFr4bmxY6PFGIWWOA3J4z/jneiB1W87YGsSxoiJ1oAvUlVtS0rEyFB+vphai7VcHSKiGMNgJtH4GeEIKhbK0TcmvJ7qcAlskPB7LHa9dAtK0JDbCd1XPBV4SkzLiIfWgC8SdWcCJWcPG8aAhohIBYOZROM5wqEnoImRgnGvVNvR6tBeRds3s1ajpLQtbAf2Bw5ktI54aAn4kpOBo0dD/hyqAu23FEtTfkREMYYJwIlKa0n6GEl2rakB0tOVbbfeCrz7rsYT+Cu45y/JOYQCfUEFW3WkNTm7tDS0jSy56omILIYJwBSY3Q4cOCBujEVF6sfESDn6F1/0DWT+8x8dgUwoIx52u7hAsM+tdaREy6hQJHN1mIdDRHGMwUwik4ucPfccsGKFGIHxZFbxtUYnToh4atw4d9vIkSL+uOwyHScKdXVSZmbgQEVrcrTWPJhI5eowD4eI4hyDGRI8R2qKi8XP/UFyUCJo/nygTRtl2549wNtv6zyR0wmsX6/tWO8RDyNGSvSMCgXL1bHZxH5H/fpp65fe6xMRWRR3zSY3eaTGRMePA23bKttGjQIWL1Y9PDCteUEy7xEPI0ZK9IwK9e8vorhhw9x7QslCnfLTe30iIgviyEy8sXCxtb/+1TeQ2bcvjEBGbWpFjb8RDyNGSvSO7hi935LZNXOIiKKAIzPxxGo7YTf66SegXTtl25gxwKuvhnjCQFMr3gKNeMjL2MMZKQlldMduFzt0G7HyyMyaOUREUcKl2fFC79LjGPHMM8CUKcq2778HLrwwjJPq2X8qL08EJMHq03gHiVreB7j3eaqoUA+uIr303ezrExGFgUuzE4kFkzyrq8V91DOQGTdOdDesQAbQPmXyxBNiDqu21ndKznO6rm1bEWGFkhwdqEhhNJa+m319IqIoYDATD2JhY0Qdnn4ayMpSth04ADz/vEEX0DplsmgRMHCgb90VtZosXboAx44BI0aIRFk9N3+j82D0Mvv6REQRxmmmeLB0qbjpBlNcLG7G0eRRdbYqpRNyhvZVvFxUJMrcGH7NQFMr/njnxXi/BoR38ze7Aq/Z1yci0knr/ZsJwPEgVpM8PXJNpmE6/gRlIFVWJlJPDBcocTeQQMdJkjhXUZFIzg0lCDB76bvZ1yciihBOM8WDSBRbC1djQvLhg/WwQcKfMM310h8wB9Kjj0UmkJH5m1rJzAz9nDE2XUdERAKDmXgQa0mejQnJT0hPowOUybgH0RFz8BgwZ44INiJJraqxEXNarMlCRBRTGMzEixhK8qwo2QbbwXLMwhOutsfxZ0iwoSMOuQ986KHIr7CSp1bkxF3v7ycUrMlCRBRTmDNjdd5Jnd9/D2zZYlqS56OPAnPn9lG0HUJ7tEel78HV1dEvoy9PyelNDgbcNVmiOV1HRERBcWTGyoxeQhyG8nJxr5871932FGZAgk09kJFFe8pGy5RcoNdYk4WIKOYwmLEqf3sPVVSI9pKSqHWlqAjo1EnZVtn2cszA9OBvNmPKJtCU3IoV4hED03VERKQN68xYkVxHxV+hvCiVqD9wALjgAmXb//4v8OSTAJYvB269NfAJ8vLMLaMfqO4Ka7IQEZmOdWbimZ6KvxHKRxk3DnjxRWXbkSMeK5+HDxcJNHPmqJ/AZjN/yiZQ3RXWZCEisgxOM1mR1jyTCOSj/PCDiEM8A5m//EXETz4lXGbPFiM03i/k5XHKhoiIDMORGSsyqeLv2LFiOyNPR48CGRkB3jRsGHDLLZyyISKiiGEwY0XBlhcbvIR43z7gkkuUbXPnApMnazwBp2yIiCiCGMxYUaC9hwxeQjxqFPDGG8q2Y8eANm3CPnVsYwIwEZFlMGfGqiJc8XfvXhEXeQYy8+eLuCnuAxm1+j2dO0d1uTsREWnHpdlWF4ERhJEjgaVLlW0nTgDp6WGd1hrk+j3e/7OQR7yYuExEFDVa798MZsjlP/8BrrhC2fbii8CDD5rTn6iLkfo9REQkaL1/c5qJAIj6dt6BjMORQIEMoK9+DxERxQwGMwnum2/EgMPy5e62V14R9+2EG8QysX4PERGFjquZEpQkifIvq1Yp20+eBFq2NKdPpjOpfg8REYWHIzMJ6KuvgKQkZSDz+usiwEnYQAZw1+/x3jFbZrOJ6sUG1e8hIiJjMJhJIJIE/OY3QI8e7jabDTh1Crj3XtO6FTvk+j2Ab0BjcP0eIiIyDoOZBPHFF2I05sMP3W1vvAE0NADnnWdev2JOhOv3EBGR8ZgzE+ckCSgoANatc7elpIgqvi1amNevmGa3A4WFrABMRGQRDGbi2PbtQK9eyrbiYmDECHP6YyncT4qIyDIYzMQhSQJuuEFZDqVVK+DIEaB5c/P6RUREFAnMmYkzW7eK3BjPQGbZMqCmhoEMERHFJ47MxImGBuC660QwI8vIACoqRI4MERFRvOLITBz49FOR4uEZyJSUAEePMpAhIqL4x5EZC2toAK69Ftixw92WkwP8+CPQrJl5/SIiIoomjsxY1MaNYjTGM5BZtUqsJGYgQ0REiYQjMxbjdAK/+IXYIFKWlwd8/z3QtKl5/SIiIjILR2Ys5JNPgCZNlIHMmjVAWRkDGSIiSlwcmbEApxPo1g3Yvdvd1qULsGePCG6IiIgSGUdmYtw//ykCFs9A5qOPgO++YyBDREQEcGQmZtXXA127ilwYWdeuYoqJQQwREZEbR2Zi0Nq1IgfGM5D5+GMxOsNAhoiISCliwcyBAwcwevRoXHDBBUhNTUWXLl0wbdo01NXVKY77+uuv0a9fPzRv3hx5eXmYPXu2z7mWL1+Orl27onnz5ujWrRvWrl0bqW5r53QCGzYAS5eKn05n2Kc8dw7o1AkYPNjd1r27GKUZMCDs0xMREcWliAUze/bsQUNDA1555RXs2rULzz33HF5++WU88cQTrmNqamowaNAgnH/++dixYwfmzJmD6dOn49VXX3Uds2XLFowYMQKjR4/Gl19+iSFDhmDIkCH49ttvI9X14EpKgM6dgfx8YORI8bNzZ9Eeog8+EPVhysvdbRs2ADt3inoyREREpM4mSZIUrYvNmTMHL730En744QcAwEsvvYSpU6eisrISzRorvU2ZMgXvv/8+9uzZAwC47bbbcPr0aaxevdp1nl/+8pfo0aMHXn75ZU3XrampQXp6OhwOB9LS0sL7ECUlwLBhYmtqTzab+Pnee4Ddrvl0dXViNKaqyt3WsyewfbvYMJKIiChRab1/R/V26XA40LZtW9fzrVu34vrrr3cFMgBQUFCAvXv34vjx465jBg4cqDhPQUEBtnpuROSltrYWNTU1iochnE5gwgTfQAZwtxUVaZ5yWrlS7J3kGchs3gx8/jkDGSIiIq2idsv87rvvsHDhQtx///2utsrKSmRnZyuOk59XVlYGPEZ+Xc2sWbOQnp7ueuTl5RnzITZvBg4e9P+6JIl5os2bA56mtlbsaO05gNO7t4iBrrvOmK4SERElCt3BzJQpU2Cz2QI+5CkiWUVFBW688UYMHz4cY8aMMazz/jz++ONwOByuR7lnIko4Dh8O+7jly4HmzYFjx9xtW7aIB0djYkAEEruJiCiydC/0nTx5MkaNGhXwmAsvvND170OHDiE/Px99+vRRJPYCQE5ODqo851gA1/OcnJyAx8ivq0lJSUFKSkrQz6Jb+/YhH3f2LJCVBZw86W674QagtNSdbkMmKykR04ieo2+5ucD8+bryoIiIKLp0BzOZmZnIzMzUdGxFRQXy8/PRs2dPLF68GEleQw+9e/fG1KlTce7cOTRt3Fxo3bp1uPTSS9GmTRvXMevXr0dRUZHrfevWrUPv3r31dj18/fqJm1tFhXrejM0mXu/XT9G8dKlY9ORp2zbg2msj2FfSx19id0WFaNeZ2E1ERNETsYmNiooK9O/fH506dcLcuXNRXV2NyspKRa7LyJEj0axZM4wePRq7du3Cu+++i/nz52PSpEmuYyZMmICPPvoIf/3rX7Fnzx5Mnz4dn3/+OcaPHx+prvuXnCz+Sgd8h1Pk5/PmudZSnzkjppQ8A5kBA4CGBgYyMcXgxG4iIooyKUIWL14sAVB9ePrqq6+k6667TkpJSZE6duwo/eUvf/E517Jly6RLLrlEatasmXTFFVdIa9as0dUXh8MhAZAcDkdYn8llxQpJys2VJHGrE4+8PNHe6O9/V74MSNLnnxtzeTJYaanvL0vtUVpqdk+JiBKK1vt3VOvMmMXQOjMyp1OsWjp8WOTI9OsHJCfj9GmgVSvlH/k33QSsWcPcmJilNg+oprgYGDEi8v0hIiIA2u/f3OknVMnJQP/+iqbFi4Hf/1552JdfAj16RK1XFIowEruJiMh8DGYMcOqUGI3xVFgoiuJxNMYCQkzsJiKi2MDKJmFatMg3kPn6a+D99xnIWIbOxG4iIootDGbCMGsWMHas+/nw4eIP+27dzOsThchuF8uvO3ZUtufmclk2EVGM4zRTGP7zH/e/d+0CLr/cvL6QAex2MT+okthNRESxi6uZwnDmDPDDD8AVVxh2SiIiImoUk7tmx5vUVAYyREREZmMwQ0RERJbGYIaIiIgsjcEMERERWRqDGSIiIrI0BjNERERkaQxmiIiIyNIYzBAREZGlMZghIiIiS2MwQ0RERJbGYIaIiIgsjcEMERERWRqDGSIiIrI0BjNERERkaQxmiIiIyNIYzBAREZGlMZghIiIiS2MwQ0RERJbGYIaIiIgsjcEMERERWRqDGSIiIrI0BjNERERkaQxmiIiIyNKamN2BhOF0Aps3A4cPA+3bA/36AcnJZveKiIjI8hjMRENJCTBhAnDwoLstNxeYPx+w283rFxERURzgNFOklZQAw4YpAxkAqKgQ7SUl5vSLiIgoTjCYiSSnU4zISJLva3JbUZE4joiIiELCYCaSNm/2HZHxJElAebk4joiIiELCYCaSDh829jgiIiLywWAmktq3N/Y4IiIi8sFgJpL69ROrlmw29ddtNiAvTxxHREREIWEwE0nJyWL5NeAb0MjP581jvRkiIqIwMJiJNLsdeO89oGNHZXturmhnnRkiIqKwsGheNNjtQGEhKwATERFFAIOZaElOBvr3N7sXREREcYfTTERERGRpDGaIiIjI0hjMEBERkaUxmCEiIiJLYzBDRERElsZghoiIiCyNwQwRERFZGoMZIiIisjQGM0RERGRpCVEBWJIkAEBNTY3JPSEiIiKt5Pu2fB/3JyGCmZMnTwIA8vLyTO4JERER6XXy5Emkp6f7fd0mBQt34kBDQwMOHTqEVq1awWazmd2dqKmpqUFeXh7Ky8uRlpZmdnfiGr/r6OD3HD38rqOH37V/kiTh5MmT6NChA5KS/GfGJMTITFJSEnJzc83uhmnS0tL4P5Ao4XcdHfyeo4ffdfTwu1YXaERGxgRgIiIisjQGM0RERGRpDGbiWEpKCqZNm4aUlBSzuxL3+F1HB7/n6OF3HT38rsOXEAnAREREFL84MkNERESWxmCGiIiILI3BDBEREVkagxkiIiKyNAYzCeDAgQMYPXo0LrjgAqSmpqJLly6YNm0a6urqzO5aXJo5cyb69OmDFi1aoHXr1mZ3J6688MIL6Ny5M5o3b45evXph+/btZncp7mzatAk333wzOnToAJvNhvfff9/sLsWtWbNm4ZprrkGrVq2QlZWFIUOGYO/evWZ3y5IYzCSAPXv2oKGhAa+88gp27dqF5557Di+//DKeeOIJs7sWl+rq6jB8+HA8+OCDZnclrrz77ruYNGkSpk2bhi+++ALdu3dHQUEBjhw5YnbX4srp06fRvXt3vPDCC2Z3Je5t3LgR48aNw2effYZ169bh3LlzGDRoEE6fPm121yyHS7MT1Jw5c/DSSy/hhx9+MLsrcWvJkiUoKirCiRMnzO5KXOjVqxeuueYaPP/88wDEnmt5eXl4+OGHMWXKFJN7F59sNhtWrlyJIUOGmN2VhFBdXY2srCxs3LgR119/vdndsRSOzCQoh8OBtm3bmt0NIk3q6uqwY8cODBw40NWWlJSEgQMHYuvWrSb2jMg4DocDAPj/zSFgMJOAvvvuOyxcuBD333+/2V0h0uTo0aNwOp3Izs5WtGdnZ6OystKkXhEZp6GhAUVFRejbty+uvPJKs7tjOQxmLGzKlCmw2WwBH3v27FG8p6KiAjfeeCOGDx+OMWPGmNRz6wnluyYi0mrcuHH49ttv8c4775jdFUtqYnYHKHSTJ0/GqFGjAh5z4YUXuv596NAh5Ofno0+fPnj11Vcj3Lv4ove7JmO1a9cOycnJqKqqUrRXVVUhJyfHpF4RGWP8+PFYvXo1Nm3ahNzcXLO7Y0kMZiwsMzMTmZmZmo6tqKhAfn4+evbsicWLFyMpiYNyeuj5rsl4zZo1Q8+ePbF+/XpXMmpDQwPWr1+P8ePHm9s5ohBJkoSHH34YK1euxIYNG3DBBReY3SXLYjCTACoqKtC/f3+cf/75mDt3Lqqrq12v8a9a45WVleHYsWMoKyuD0+nEzp07AQAXXXQRWrZsaW7nLGzSpEm45557cPXVV+Paa6/FvHnzcPr0adx7771mdy2unDp1Ct99953r+f79+7Fz5060bdsWnTp1MrFn8WfcuHEoLi7GqlWr0KpVK1f+V3p6OlJTU03uncVIFPcWL14sAVB9kPHuuece1e+6tLTU7K5Z3sKFC6VOnTpJzZo1k6699lrps88+M7tLcae0tFT1v9977rnH7K7FHX//v7x48WKzu2Y5rDNDRERElsbECSIiIrI0BjNERERkaQxmiIiIyNIYzBAREZGlMZghIiIiS2MwQ0RERJbGYIaIiIgsjcEMERERWRqDGSIiIrI0BjNERERkaQxmiIiIyNIYzBAREZGl/X+q0uLPrdwMEwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 518 ms, sys: 47 ms, total: 565 ms\n",
            "Wall time: 950 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression using PyTorch"
      ],
      "metadata": {
        "id": "ZyvpiJHJJUuO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4badV2LKG32J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}